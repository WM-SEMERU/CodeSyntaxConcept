{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch1/svelascodimate/miniconda3/envs/CodeSyntaxConcept/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import CodeSyntaxConcept\n",
    "\n",
    "from CodeSyntaxConcept.tokenizer import CodeTokenizer\n",
    "from CodeSyntaxConcept.parser import TreeSitterParser\n",
    "import CodeSyntaxConcept.utils as utils\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Evaluator:\n",
    "\n",
    "    def __init__(self, checkpoint: str, language):\n",
    "        self.tokenizer = CodeTokenizer.from_pretrained(checkpoint, language)\n",
    "        self.parser = TreeSitterParser(self.tokenizer)\n",
    "    \n",
    "    def __call__(self, test_set):\n",
    "        test_set_concepts = pd.DataFrame([], columns=['whole_func_string', 'ast_concepts', 'model_tokenizer_concepts', 'model_input_ids', 'model_total_input_ids'])\n",
    "        for test_sample in test_set: \n",
    "            ast_concepts = self.parser.process_source_code(test_sample['whole_func_string'])\n",
    "            source_code_encoding, tokenizer_concepts =  self.parser.process_model_source_code(test_sample['whole_func_string'])\n",
    "            test_set_concepts.loc[len(test_set_concepts.index)] = (test_sample['whole_func_string'], ast_concepts, tokenizer_concepts, source_code_encoding['input_ids'], len(source_code_encoding['input_ids']))\n",
    "        return test_set_concepts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: code_search_net/all\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset code_search_net/all to /home/svelascodimate/.cache/huggingface/datasets/code_search_net/all/1.0.0/80a244ab541c6b2125350b764dc5c2b715f65f00de7a56107a28915fac173a27...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 6/6 [00:00<00:00, 4808.14it/s]\n",
      "Extracting data files: 100%|██████████| 6/6 [00:00<00:00, 1683.78it/s]\n"
     ]
    },
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: '/home/svelascodimate/.cache/huggingface/datasets/downloads/25ceeb4c25ab737d688bd56ea92bfbb1f199fe572470456cf2d675479f342ac7/python/final/jsonl/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m maximun_number_of_samples \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m     10\u001b[0m evaluator \u001b[39m=\u001b[39m Evaluator(checkpoint, language)\n\u001b[0;32m---> 12\u001b[0m test_set \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mget_random_sub_set_test_set(utils\u001b[39m.\u001b[39mget_test_sets(load_dataset(\u001b[39m\"\u001b[39m\u001b[39mcode_search_net\u001b[39m\u001b[39m\"\u001b[39m, split\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mpython\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m2048\u001b[39m, evaluator\u001b[39m.\u001b[39mtokenizer), maximun_number_of_samples)\n",
      "File \u001b[0;32m/scratch1/svelascodimate/miniconda3/envs/CodeSyntaxConcept/lib/python3.11/site-packages/datasets/load.py:1797\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1794\u001b[0m try_from_hf_gcs \u001b[39m=\u001b[39m path \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[1;32m   1796\u001b[0m \u001b[39m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 1797\u001b[0m builder_instance\u001b[39m.\u001b[39mdownload_and_prepare(\n\u001b[1;32m   1798\u001b[0m     download_config\u001b[39m=\u001b[39mdownload_config,\n\u001b[1;32m   1799\u001b[0m     download_mode\u001b[39m=\u001b[39mdownload_mode,\n\u001b[1;32m   1800\u001b[0m     verification_mode\u001b[39m=\u001b[39mverification_mode,\n\u001b[1;32m   1801\u001b[0m     try_from_hf_gcs\u001b[39m=\u001b[39mtry_from_hf_gcs,\n\u001b[1;32m   1802\u001b[0m     num_proc\u001b[39m=\u001b[39mnum_proc,\n\u001b[1;32m   1803\u001b[0m     storage_options\u001b[39m=\u001b[39mstorage_options,\n\u001b[1;32m   1804\u001b[0m )\n\u001b[1;32m   1806\u001b[0m \u001b[39m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   1807\u001b[0m keep_in_memory \u001b[39m=\u001b[39m (\n\u001b[1;32m   1808\u001b[0m     keep_in_memory \u001b[39mif\u001b[39;00m keep_in_memory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m is_small_dataset(builder_instance\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size)\n\u001b[1;32m   1809\u001b[0m )\n",
      "File \u001b[0;32m/scratch1/svelascodimate/miniconda3/envs/CodeSyntaxConcept/lib/python3.11/site-packages/datasets/builder.py:890\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    888\u001b[0m     \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    889\u001b[0m         prepare_split_kwargs[\u001b[39m\"\u001b[39m\u001b[39mnum_proc\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m num_proc\n\u001b[0;32m--> 890\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download_and_prepare(\n\u001b[1;32m    891\u001b[0m         dl_manager\u001b[39m=\u001b[39mdl_manager,\n\u001b[1;32m    892\u001b[0m         verification_mode\u001b[39m=\u001b[39mverification_mode,\n\u001b[1;32m    893\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprepare_split_kwargs,\n\u001b[1;32m    894\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdownload_and_prepare_kwargs,\n\u001b[1;32m    895\u001b[0m     )\n\u001b[1;32m    896\u001b[0m \u001b[39m# Sync info\u001b[39;00m\n\u001b[1;32m    897\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(split\u001b[39m.\u001b[39mnum_bytes \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39msplits\u001b[39m.\u001b[39mvalues())\n",
      "File \u001b[0;32m/scratch1/svelascodimate/miniconda3/envs/CodeSyntaxConcept/lib/python3.11/site-packages/datasets/builder.py:1649\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_download_and_prepare\u001b[39m(\u001b[39mself\u001b[39m, dl_manager, verification_mode, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprepare_splits_kwargs):\n\u001b[0;32m-> 1649\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m_download_and_prepare(\n\u001b[1;32m   1650\u001b[0m         dl_manager,\n\u001b[1;32m   1651\u001b[0m         verification_mode,\n\u001b[1;32m   1652\u001b[0m         check_duplicate_keys\u001b[39m=\u001b[39mverification_mode \u001b[39m==\u001b[39m VerificationMode\u001b[39m.\u001b[39mBASIC_CHECKS\n\u001b[1;32m   1653\u001b[0m         \u001b[39mor\u001b[39;00m verification_mode \u001b[39m==\u001b[39m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS,\n\u001b[1;32m   1654\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprepare_splits_kwargs,\n\u001b[1;32m   1655\u001b[0m     )\n",
      "File \u001b[0;32m/scratch1/svelascodimate/miniconda3/envs/CodeSyntaxConcept/lib/python3.11/site-packages/datasets/builder.py:963\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m split_dict \u001b[39m=\u001b[39m SplitDict(dataset_name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m    962\u001b[0m split_generators_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_split_generators_kwargs(prepare_split_kwargs)\n\u001b[0;32m--> 963\u001b[0m split_generators \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split_generators(dl_manager, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msplit_generators_kwargs)\n\u001b[1;32m    965\u001b[0m \u001b[39m# Checksums verification\u001b[39;00m\n\u001b[1;32m    966\u001b[0m \u001b[39mif\u001b[39;00m verification_mode \u001b[39m==\u001b[39m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS \u001b[39mand\u001b[39;00m dl_manager\u001b[39m.\u001b[39mrecord_checksums:\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/datasets_modules/datasets/code_search_net/80a244ab541c6b2125350b764dc5c2b715f65f00de7a56107a28915fac173a27/code_search_net.py:166\u001b[0m, in \u001b[0;36mCodeSearchNet._split_generators\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m    155\u001b[0m data_dirs \u001b[39m=\u001b[39m [\n\u001b[1;32m    156\u001b[0m     os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(directory, lang, \u001b[39m\"\u001b[39m\u001b[39mfinal\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mjsonl\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    157\u001b[0m     \u001b[39mfor\u001b[39;00m lang, directory \u001b[39min\u001b[39;00m dl_manager\u001b[39m.\u001b[39mdownload_and_extract(data_urls)\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    158\u001b[0m ]\n\u001b[1;32m    160\u001b[0m split2dirs \u001b[39m=\u001b[39m {\n\u001b[1;32m    161\u001b[0m     split_name: [os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(directory, split_name) \u001b[39mfor\u001b[39;00m directory \u001b[39min\u001b[39;00m data_dirs]\n\u001b[1;32m    162\u001b[0m     \u001b[39mfor\u001b[39;00m split_name \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    163\u001b[0m }\n\u001b[1;32m    165\u001b[0m split2paths \u001b[39m=\u001b[39m dl_manager\u001b[39m.\u001b[39mextract(\n\u001b[0;32m--> 166\u001b[0m     {\n\u001b[1;32m    167\u001b[0m         split_name: [\n\u001b[1;32m    168\u001b[0m             os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(directory, entry_name)\n\u001b[1;32m    169\u001b[0m             \u001b[39mfor\u001b[39;00m directory \u001b[39min\u001b[39;00m split_dirs\n\u001b[1;32m    170\u001b[0m             \u001b[39mfor\u001b[39;00m entry_name \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(directory)\n\u001b[1;32m    171\u001b[0m         ]\n\u001b[1;32m    172\u001b[0m         \u001b[39mfor\u001b[39;00m split_name, split_dirs \u001b[39min\u001b[39;00m split2dirs\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    173\u001b[0m     }\n\u001b[1;32m    174\u001b[0m )\n\u001b[1;32m    176\u001b[0m \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    177\u001b[0m     datasets\u001b[39m.\u001b[39mSplitGenerator(\n\u001b[1;32m    178\u001b[0m         name\u001b[39m=\u001b[39mdatasets\u001b[39m.\u001b[39mSplit\u001b[39m.\u001b[39mTRAIN,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m     ),\n\u001b[1;32m    195\u001b[0m ]\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/datasets_modules/datasets/code_search_net/80a244ab541c6b2125350b764dc5c2b715f65f00de7a56107a28915fac173a27/code_search_net.py:167\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    155\u001b[0m data_dirs \u001b[39m=\u001b[39m [\n\u001b[1;32m    156\u001b[0m     os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(directory, lang, \u001b[39m\"\u001b[39m\u001b[39mfinal\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mjsonl\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    157\u001b[0m     \u001b[39mfor\u001b[39;00m lang, directory \u001b[39min\u001b[39;00m dl_manager\u001b[39m.\u001b[39mdownload_and_extract(data_urls)\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    158\u001b[0m ]\n\u001b[1;32m    160\u001b[0m split2dirs \u001b[39m=\u001b[39m {\n\u001b[1;32m    161\u001b[0m     split_name: [os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(directory, split_name) \u001b[39mfor\u001b[39;00m directory \u001b[39min\u001b[39;00m data_dirs]\n\u001b[1;32m    162\u001b[0m     \u001b[39mfor\u001b[39;00m split_name \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    163\u001b[0m }\n\u001b[1;32m    165\u001b[0m split2paths \u001b[39m=\u001b[39m dl_manager\u001b[39m.\u001b[39mextract(\n\u001b[1;32m    166\u001b[0m     {\n\u001b[0;32m--> 167\u001b[0m         split_name: [\n\u001b[1;32m    168\u001b[0m             os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(directory, entry_name)\n\u001b[1;32m    169\u001b[0m             \u001b[39mfor\u001b[39;00m directory \u001b[39min\u001b[39;00m split_dirs\n\u001b[1;32m    170\u001b[0m             \u001b[39mfor\u001b[39;00m entry_name \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(directory)\n\u001b[1;32m    171\u001b[0m         ]\n\u001b[1;32m    172\u001b[0m         \u001b[39mfor\u001b[39;00m split_name, split_dirs \u001b[39min\u001b[39;00m split2dirs\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    173\u001b[0m     }\n\u001b[1;32m    174\u001b[0m )\n\u001b[1;32m    176\u001b[0m \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    177\u001b[0m     datasets\u001b[39m.\u001b[39mSplitGenerator(\n\u001b[1;32m    178\u001b[0m         name\u001b[39m=\u001b[39mdatasets\u001b[39m.\u001b[39mSplit\u001b[39m.\u001b[39mTRAIN,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m     ),\n\u001b[1;32m    195\u001b[0m ]\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/datasets_modules/datasets/code_search_net/80a244ab541c6b2125350b764dc5c2b715f65f00de7a56107a28915fac173a27/code_search_net.py:170\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    155\u001b[0m data_dirs \u001b[39m=\u001b[39m [\n\u001b[1;32m    156\u001b[0m     os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(directory, lang, \u001b[39m\"\u001b[39m\u001b[39mfinal\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mjsonl\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    157\u001b[0m     \u001b[39mfor\u001b[39;00m lang, directory \u001b[39min\u001b[39;00m dl_manager\u001b[39m.\u001b[39mdownload_and_extract(data_urls)\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    158\u001b[0m ]\n\u001b[1;32m    160\u001b[0m split2dirs \u001b[39m=\u001b[39m {\n\u001b[1;32m    161\u001b[0m     split_name: [os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(directory, split_name) \u001b[39mfor\u001b[39;00m directory \u001b[39min\u001b[39;00m data_dirs]\n\u001b[1;32m    162\u001b[0m     \u001b[39mfor\u001b[39;00m split_name \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    163\u001b[0m }\n\u001b[1;32m    165\u001b[0m split2paths \u001b[39m=\u001b[39m dl_manager\u001b[39m.\u001b[39mextract(\n\u001b[1;32m    166\u001b[0m     {\n\u001b[1;32m    167\u001b[0m         split_name: [\n\u001b[1;32m    168\u001b[0m             os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(directory, entry_name)\n\u001b[1;32m    169\u001b[0m             \u001b[39mfor\u001b[39;00m directory \u001b[39min\u001b[39;00m split_dirs\n\u001b[0;32m--> 170\u001b[0m             \u001b[39mfor\u001b[39;00m entry_name \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(directory)\n\u001b[1;32m    171\u001b[0m         ]\n\u001b[1;32m    172\u001b[0m         \u001b[39mfor\u001b[39;00m split_name, split_dirs \u001b[39min\u001b[39;00m split2dirs\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    173\u001b[0m     }\n\u001b[1;32m    174\u001b[0m )\n\u001b[1;32m    176\u001b[0m \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    177\u001b[0m     datasets\u001b[39m.\u001b[39mSplitGenerator(\n\u001b[1;32m    178\u001b[0m         name\u001b[39m=\u001b[39mdatasets\u001b[39m.\u001b[39mSplit\u001b[39m.\u001b[39mTRAIN,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m     ),\n\u001b[1;32m    195\u001b[0m ]\n",
      "File \u001b[0;32m/scratch1/svelascodimate/miniconda3/envs/CodeSyntaxConcept/lib/python3.11/site-packages/datasets/streaming.py:71\u001b[0m, in \u001b[0;36mextend_module_for_streaming.<locals>.wrap_auth.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39m@wraps\u001b[39m(function)\n\u001b[1;32m     70\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 71\u001b[0m     \u001b[39mreturn\u001b[39;00m function(\u001b[39m*\u001b[39margs, use_auth_token\u001b[39m=\u001b[39muse_auth_token, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/scratch1/svelascodimate/miniconda3/envs/CodeSyntaxConcept/lib/python3.11/site-packages/datasets/download/streaming_download_manager.py:524\u001b[0m, in \u001b[0;36mxlistdir\u001b[0;34m(path, use_auth_token)\u001b[0m\n\u001b[1;32m    522\u001b[0m main_hop, \u001b[39m*\u001b[39mrest_hops \u001b[39m=\u001b[39m _as_str(path)\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m::\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    523\u001b[0m \u001b[39mif\u001b[39;00m is_local_path(main_hop):\n\u001b[0;32m--> 524\u001b[0m     \u001b[39mreturn\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(path)\n\u001b[1;32m    525\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    526\u001b[0m     \u001b[39m# globbing inside a zip in a private repo requires authentication\u001b[39;00m\n\u001b[1;32m    527\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m rest_hops \u001b[39mand\u001b[39;00m (main_hop\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mhttp://\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m main_hop\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mhttps://\u001b[39m\u001b[39m\"\u001b[39m)):\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '/home/svelascodimate/.cache/huggingface/datasets/downloads/25ceeb4c25ab737d688bd56ea92bfbb1f199fe572470456cf2d675479f342ac7/python/final/jsonl/train'"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "\n",
    "from datasets import load_dataset    \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "checkpoint = \"EleutherAI/gpt-neo-125M\"\n",
    "language = \"python\"\n",
    "maximun_number_of_samples = 2\n",
    "\n",
    "evaluator = Evaluator(checkpoint, language)\n",
    "\n",
    "test_set = utils.get_random_sub_set_test_set(utils.get_test_sets(load_dataset(\"code_search_net\", split='test'), \"python\", 2048, evaluator.tokenizer), maximun_number_of_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       model_total_input_ids\n",
      "count               2.000000\n",
      "mean              896.000000\n",
      "std              1120.057141\n",
      "min               104.000000\n",
      "25%               500.000000\n",
      "50%               896.000000\n",
      "75%              1292.000000\n",
      "max              1688.000000\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "print(evaluator(test_set).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "print(evaluator.tokenizer.tokenizer.max_len_single_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

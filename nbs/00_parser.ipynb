{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import CodeSyntaxConcept\n",
    "\n",
    "from CodeSyntaxConcept.tokenizer import CodeTokenizer, get_token_type\n",
    "import CodeSyntaxConcept.utils as utils\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TreeSitterParser:\n",
    "    \"\"\"Class to parse source code snippets using the Tree-sitter grammar\"\"\"\n",
    "    def __init__(self, tokenizer: CodeTokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def process_source_code(self, \n",
    "                            source_code: str #Source code to parse\n",
    "                            ):\n",
    "        \"\"\"\"Process the given source code snippet and gets the ast node types\"\"\"\n",
    "        ast_representation = self.tokenizer.parser.parse(bytes(source_code, \"utf8\"))\n",
    "        ast_nodes = []\n",
    "        utils.traverse(ast_representation.root_node, ast_nodes)\n",
    "        source_code_ast_types = []\n",
    "        for node_index, node in enumerate(ast_nodes):\n",
    "            #source_code_ast_types.append([node.text.decode(\"utf-8\"), self.tokenizer.node_types.index(node.type), self.tokenizer.node_types.index(node.parent.type)])\n",
    "            source_code_ast_types.append((node.text.decode(\"utf-8\"), node.type, node.parent.type))\n",
    "        return source_code_ast_types\n",
    "\n",
    "    def process_model_source_code(self, \n",
    "                                  source_code: str # Source code to process\n",
    "                                  ):\n",
    "        \"\"\"Process the given source code snippet and gets the encodings and ast types\"\"\"\n",
    "        source_code_encoding = self.tokenizer(source_code)\n",
    "        source_code_ast_types = []\n",
    "        for input_id_index, input_id in enumerate(source_code_encoding['input_ids']):\n",
    "            #source_code_ast_types.append([input_id, source_code_encoding['ast_ids'][input_id_index], source_code_encoding['parent_ast_ids'][input_id_index]])\n",
    "            source_code_ast_types.append((\n",
    "                input_id, \n",
    "                'ERROR' if source_code_encoding['ast_ids'][input_id_index] == -1 else self.tokenizer.node_types[source_code_encoding['ast_ids'][input_id_index]], \n",
    "                'ERROR' if source_code_encoding['ast_ids'][input_id_index] == -1 else self.tokenizer.node_types[source_code_encoding['parent_ast_ids'][input_id_index]]))\n",
    "        return source_code_encoding, source_code_ast_types"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "checkpoint = \"EleutherAI/gpt-neo-125M\"\n",
    "#checkpoint = \"EleutherAI/gpt-neo-1.3B\"\n",
    "\n",
    "tokenizer = CodeTokenizer.from_pretrained(checkpoint, \"python\")\n",
    "parser = TreeSitterParser(tokenizer)\n",
    "\n",
    "source_code = \"def multiply_numbers(a,b):\\n    return a*b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('def', 'def', 'function_definition'), ('multiply_numbers', 'identifier', 'function_definition'), ('(', '(', 'parameters'), ('a', 'identifier', 'parameters'), (',', ',', 'parameters'), ('b', 'identifier', 'parameters'), (')', ')', 'parameters'), (':', ':', 'function_definition'), ('return', 'return', 'return_statement'), ('a', 'identifier', 'binary_operator'), ('*', '*', 'binary_operator'), ('b', 'identifier', 'binary_operator')]\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "print(parser.process_source_code(source_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4299, 29162, 62, 77, 17024, 7, 64, 11, 65, 2599, 198, 220, 220, 220, 1441, 257, 9, 65]\n",
      "[(4299, 'def', 'function_definition'), (29162, 'identifier', 'function_definition'), (62, 'identifier', 'function_definition'), (77, 'identifier', 'function_definition'), (17024, 'identifier', 'function_definition'), (7, '(', 'parameters'), (64, 'identifier', 'parameters'), (11, ',', 'parameters'), (65, 'identifier', 'parameters'), (2599, ')', 'parameters'), (198, 'ERROR', 'ERROR'), (220, 'ERROR', 'ERROR'), (220, 'ERROR', 'ERROR'), (220, 'ERROR', 'ERROR'), (1441, 'return', 'return_statement'), (257, 'identifier', 'binary_operator'), (9, '*', 'binary_operator'), (65, 'identifier', 'binary_operator')]\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "source_code_encoding, source_code_dataframe = parser.process_model_source_code(source_code)\n",
    "print(source_code_encoding['input_ids'])\n",
    "print(source_code_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

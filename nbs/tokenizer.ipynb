{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Module Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/svelascodimate/miniconda3/envs/code-syntax-concept/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import CodeSyntaxConcept\n",
    "import CodeSyntaxConcept.utils as utils\n",
    "\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "from tree_sitter import Language, Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_token_type(\n",
    "    tok_span: tuple, # (start, end) position of a token in tokenizer\n",
    "    nodes: list,     # list of tree-sitter nodes\n",
    "    lines: list,     # list of lines in the code\n",
    ") -> tuple: # (parent_type, token_type) of the token\n",
    "    \"\"\"Get the parent AST type and token AST type of a token.\"\"\"\n",
    "    def get_node_span(node):\n",
    "        start_span = utils.convert_to_offset(node.start_point, lines)\n",
    "        end_span = utils.convert_to_offset(node.end_point, lines)\n",
    "        return start_span, end_span\n",
    "    \n",
    "    node_spans = [get_node_span(node) for node in nodes]\n",
    "    for i, span in enumerate(node_spans):\n",
    "        if (span[0] <= tok_span[0] and tok_span[0] < span[1]) or (span[0] < tok_span[1] and tok_span[1] <= span[1]):\n",
    "            return nodes[i].parent.type, nodes[i].type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CodeTokenizer():\n",
    "    \"\"\"A tokenizer for code, which aligns the tokens with the AST nodes.\"\"\"\n",
    "    def __init__(self, tokenizer, parser, node_types):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.parser = parser\n",
    "        self.node_types = node_types\n",
    "    \n",
    "    def __call__(self, code):\n",
    "        encoding = self.tokenizer(code, return_offsets_mapping=True)\n",
    "        tree = self.parser.parse(bytes(code, \"utf8\"))\n",
    "        nodes = []\n",
    "        utils.traverse(tree.root_node, nodes)\n",
    "\n",
    "        encoding[\"ast_ids\"] = []\n",
    "        encoding[\"parent_ast_ids\"] = []\n",
    "        for i, (start, end) in enumerate(encoding.offset_mapping):\n",
    "            if encoding[\"input_ids\"][i] in self.tokenizer.all_special_ids:\n",
    "                encoding[\"ast_ids\"].append(-1)\n",
    "                encoding[\"parent_ast_ids\"].append(-1)\n",
    "                continue\n",
    "            if start == None or end == None:\n",
    "                encoding[\"ast_ids\"].append(-1)\n",
    "                encoding[\"parent_ast_ids\"].append(-1)\n",
    "                continue\n",
    "            type_info = get_token_type((start, end), nodes, code.split(\"\\n\"))\n",
    "            if type_info is None:\n",
    "                encoding[\"ast_ids\"].append(-1)\n",
    "                encoding[\"parent_ast_ids\"].append(-1)\n",
    "            else:\n",
    "                parent_node_type, node_type = type_info\n",
    "                try:\n",
    "                    encoding[\"ast_ids\"].append(self.node_types.index(node_type))\n",
    "                    encoding[\"parent_ast_ids\"].append(self.node_types.index(parent_node_type))\n",
    "                except Exception as e:\n",
    "                    print(type_info)\n",
    "                    print(code)\n",
    "                    print(self.tokenizer.decode(encoding[\"input_ids\"][i]))\n",
    "                    encoding[\"ast_ids\"].append(-1)\n",
    "                    encoding[\"parent_ast_ids\"].append(-1)\n",
    "                    raise e\n",
    "            \n",
    "        return encoding\n",
    "\n",
    "    @staticmethod\n",
    "    def from_pretrained(\n",
    "        name_or_path: str,  # name or path of the tokenizer\n",
    "        lang: str,          # language of the tokenizer\n",
    "    ):                      # CodeTokenizer for the given language\n",
    "        \"\"\"Create a CodeTokenizer from a pretrained tokenizer for a given language.\"\"\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(name_or_path)\n",
    "\n",
    "        # Grab the node types from the tree-sitter language\n",
    "        language = Language(f\"{CodeSyntaxConcept.__path__[0]}/grammars/tree-sitter-languages.so\", lang)\n",
    "        node_path = f\"{CodeSyntaxConcept.__path__[0]}/grammars/tree-sitter-{lang}/src/node-types.json\"\n",
    "        with open(node_path) as f:\n",
    "            node_types = json.load(f)\n",
    "        node_types = utils.unroll_node_types(node_types)\n",
    "\n",
    "        # Create a parser for the language\n",
    "        parser = Parser()\n",
    "        parser.set_language(language)\n",
    "        \n",
    "        return CodeTokenizer(tokenizer, parser, node_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/svelascodimate/miniconda3/envs/code-syntax-concept/lib/python3.10/site-packages/CodeSyntaxConcept/grammars\n"
     ]
    }
   ],
   "source": [
    "from CodeSyntaxConcept import loader\n",
    "\n",
    "\"\"\"define language\"\"\"\n",
    "python_language = \"python\"\n",
    "\n",
    "languages = [python_language]\n",
    "\n",
    "loader.download_grammars(languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [4299, 22944, 33529, 198, 220, 220, 220, 3601, 10786, 31373, 995, 11537], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 3), (3, 7), (7, 10), (10, 11), (11, 12), (12, 13), (13, 14), (14, 20), (20, 22), (22, 27), (27, 33), (33, 35)], 'ast_ids': [161, 108, 16, -1, -1, -1, -1, 108, 16, 137, 137, 137], 'parent_ast_ids': [122, 122, 38, -1, -1, -1, -1, 8, 58, 58, 58, 58]}\n",
      "['def', 'Ġfoo', '():', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġprint', \"('\", 'hello', 'Ġworld', \"')\"]\n"
     ]
    }
   ],
   "source": [
    "# test the tokenizer\n",
    "py_tokenizer = CodeTokenizer.from_pretrained(\"microsoft/CodeGPT-small-py\", \"python\")\n",
    "#code = \"def foo():\\n    print('hello world')\"\n",
    "#code =  \"def reserve_free_tcp_port(self):\\n        \\\"\\\"\\\"\\\"\\\"\\\"\\n        Reserve free TCP port to be used by Cloud SQL Proxy\\n        \\\"\\\"\\\"\\\"\\\"\\\"\\n        self.reserved_tcp_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\\n        self.reserved_tcp_socket.bind(('127.0.0.1', 0))\\n        self.sql_proxy_tcp_port = self.reserved_tcp_socket.getsockname()[1]\"\n",
    "\n",
    "code = 'def bubblesort(elements):\\n    swapped = False\\n    # Looping from size of array from last index[-1] to index [0]\\n    for n in range(len(elements)-1, 0, -1):\\n        for i in range(n):\\n            if elements[i] > elements[i + 1]:\\n                swapped = True\\n                # swapping data if the element is less than next element in the array\\n                elements[i], elements[i + 1] = elements[i + 1], elements[i]       \\n        if not swapped:\\n            # exiting the function if we didn\\'t make a single swap\\n            # meaning that the array is already sorted.\\n            return'\n",
    "\n",
    "#print(py_tokenizer.node_types)\n",
    "\n",
    "encoding = py_tokenizer(code)\n",
    "\n",
    "assert \"ast_ids\" in encoding\n",
    "assert \"parent_ast_ids\" in encoding\n",
    "assert len(encoding[\"ast_ids\"]) == len(encoding[\"input_ids\"])\n",
    "assert len(encoding[\"parent_ast_ids\"]) == len(encoding[\"input_ids\"])\n",
    "\n",
    "print(encoding)\n",
    "print(py_tokenizer.tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(py_tokenizer.tokenizer(code)['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positional_separator', 'dotted_name', '+=', '&', '^', 'if', 'not', '>>', 'call', 'primary_expression', '|', 'for_statement', 'from', 'float', '^=', 'lambda_parameters', '(', 'aliased_import', 'case', 'try_statement', 'list', '|=', 'typed_default_parameter', 'augmented_assignment', 'binary_operator', 'list_pattern', 'type', '{{', '<<', ')', 'break', 'del', 'for', '*', 'finally', 'generator_expression', 'import_prefix', 'comparison_operator', 'parameters', 'dictionary_splat_pattern', 'exec_statement', 'class_definition', 'future_import_statement', 'finally_clause', 'parenthesized_list_splat', 'list_splat_pattern', '<<=', 'module', 'class', '{', 'assignment', 'if_statement', '-=', 'for_in_clause', '+', '>=', 'print', '.', 'argument_list', 'yield', 'not_operator', '%', 'print_statement', 'set', 'unary_operator', 'nonlocal', 'as_pattern', '*=', 'import', 'or', 'raise_statement', 'set_comprehension', 'conditional_expression', 'elif', 'with_statement', 'expression_statement', '//', 'elif_clause', 'return_statement', 'dictionary_splat', 'else_clause', '%=', 'keyword_argument', 'is', 'dictionary', '<=', '\"', ':', 'exec', 'as', 'while', '!=', 'pair', '>>=', 'decorated_definition', 'lambda', '<>', ']', 'pass_statement', 'raise', 'pass', 'default_parameter', 'continue', 'if_clause', 'try', 'relative_import', '-', 'case_clause', 'identifier', 'ERROR', 'interpolation', 'await', 'assert_statement', 'else', 'keyword_separator', 'decorator', '}', 'and', 'assert', '<', 'parameter', '~', 'function_definition', '->', '&=', '_simple_statement', 'delete_statement', '_compound_statement', 'match_statement', 'concatenated_string', 'as_pattern_target', 'tuple', '__future__', 'tuple_pattern', 'format_expression', ',', 'continue_statement', 'string', 'block', 'except_clause', 'wildcard_import', 'typed_parameter', 'slice', 'comment', 'pattern_list', 'except', 'global_statement', '[', 'break_statement', 'in', 'none', '@=', 'global', 'subscript', 'nonlocal_statement', 'dictionary_comprehension', 'escape_sequence', 'true', '==', '**', 'list_splat', 'def', 'expression', 'with_item', 'attribute', 'import_statement', 'with', '**=', 'import_from_statement', 'expression_list', '//=', 'false', 'while_statement', 'type_conversion', 'async', 'case_pattern', '@', 'named_expression', 'integer', 'ellipsis', '>', '/', 'with_clause', ';', 'boolean_operator', ':=', 'return', '}}', 'chevron', 'match', 'parenthesized_expression', '=', 'format_specifier', 'pattern', 'list_comprehension', '/=']\n",
      "196\n"
     ]
    }
   ],
   "source": [
    "print(py_tokenizer.node_types)\n",
    "print(len(py_tokenizer.node_types))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49b68b9c38357b2088122ab1ddc655dffe83bd2309642e44c64d6d94a1d66aec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

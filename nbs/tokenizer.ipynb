{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Module Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import CodeSyntaxConcept\n",
    "import CodeSyntaxConcept.utils as utils\n",
    "\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "from tree_sitter import Language, Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_token_type(\n",
    "    tok_span: tuple, # (start, end) position of a token in tokenizer\n",
    "    nodes: list,     # list of tree-sitter nodes\n",
    "    lines: list,     # list of lines in the code\n",
    ") -> tuple: # (parent_type, token_type) of the token\n",
    "    \"\"\"Get the parent AST type and token AST type of a token.\"\"\"\n",
    "    def get_node_span(node):\n",
    "        start_span = utils.convert_to_offset(node.start_point, lines)\n",
    "        end_span = utils.convert_to_offset(node.end_point, lines)\n",
    "        return start_span, end_span\n",
    "    \n",
    "    node_spans = [get_node_span(node) for node in nodes]\n",
    "    for i, span in enumerate(node_spans):\n",
    "        if (span[0] <= tok_span[0] and tok_span[0] < span[1]) or (span[0] < tok_span[1] and tok_span[1] <= span[1]):\n",
    "            return nodes[i].parent.type, nodes[i].type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CodeTokenizer():\n",
    "    \"\"\"A tokenizer for code, which aligns the tokens with the AST nodes.\"\"\"\n",
    "    def __init__(self, tokenizer, parser, node_types):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.parser = parser\n",
    "        self.node_types = node_types\n",
    "    \n",
    "    def __call__(self, code):\n",
    "        encoding = self.tokenizer(code, return_offsets_mapping=True)\n",
    "        tree = self.parser.parse(bytes(code, \"utf8\"))\n",
    "        nodes = []\n",
    "        utils.traverse(tree.root_node, nodes)\n",
    "\n",
    "        encoding[\"ast_ids\"] = []\n",
    "        encoding[\"parent_ast_ids\"] = []\n",
    "        for i, (start, end) in enumerate(encoding.offset_mapping):\n",
    "            if encoding[\"input_ids\"][i] in self.tokenizer.all_special_ids:\n",
    "                encoding[\"ast_ids\"].append(-1)\n",
    "                encoding[\"parent_ast_ids\"].append(-1)\n",
    "                continue\n",
    "            if start == None or end == None:\n",
    "                encoding[\"ast_ids\"].append(-1)\n",
    "                encoding[\"parent_ast_ids\"].append(-1)\n",
    "                continue\n",
    "            type_info = get_token_type((start, end), nodes, code.split(\"\\n\"))\n",
    "            if type_info is None:\n",
    "                encoding[\"ast_ids\"].append(-1)\n",
    "                encoding[\"parent_ast_ids\"].append(-1)\n",
    "            else:\n",
    "                parent_node_type, node_type = type_info\n",
    "                try:\n",
    "                    encoding[\"ast_ids\"].append(self.node_types.index(node_type))\n",
    "                    encoding[\"parent_ast_ids\"].append(self.node_types.index(parent_node_type))\n",
    "                except Exception as e:\n",
    "                    print(type_info)\n",
    "                    print(code)\n",
    "                    print(self.tokenizer.decode(encoding[\"input_ids\"][i]))\n",
    "                    encoding[\"ast_ids\"].append(-1)\n",
    "                    encoding[\"parent_ast_ids\"].append(-1)\n",
    "                    raise e\n",
    "            \n",
    "        return encoding\n",
    "\n",
    "    @staticmethod\n",
    "    def from_pretrained(\n",
    "        name_or_path: str,  # name or path of the tokenizer\n",
    "        lang: str,          # language of the tokenizer\n",
    "    ):                      # CodeTokenizer for the given language\n",
    "        \"\"\"Create a CodeTokenizer from a pretrained tokenizer for a given language.\"\"\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(name_or_path)\n",
    "\n",
    "        # Grab the node types from the tree-sitter language\n",
    "        language = Language(f\"{CodeSyntaxConcept.__path__[0]}/grammars/tree-sitter-languages.so\", lang)\n",
    "        node_path = f\"{CodeSyntaxConcept.__path__[0]}/grammars/tree-sitter-{lang}/src/node-types.json\"\n",
    "        with open(node_path) as f:\n",
    "            node_types = json.load(f)\n",
    "        node_types = utils.unroll_node_types(node_types)\n",
    "\n",
    "        # Create a parser for the language\n",
    "        parser = Parser()\n",
    "        parser.set_language(language)\n",
    "        \n",
    "        return CodeTokenizer(tokenizer, parser, node_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/CodeSyntaxConcept/grammars\n"
     ]
    }
   ],
   "source": [
    "from CodeSyntaxConcept import loader\n",
    "\n",
    "\"\"\"define language\"\"\"\n",
    "python_language = \"python\"\n",
    "\n",
    "languages = [python_language]\n",
    "\n",
    "loader.download_grammars(languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bbd25c7ddf14ca5942b63e75d118ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/175 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ad521fae89465ab9ca26e09143652b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/610 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d175298d04147a89a9f2bf9bde687d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/865k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d648b00fa2de4c22b582bc383e7b4648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/424k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40099acc78924010948f2d52503cc63d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)in/added_tokens.json:   0%|          | 0.00/16.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc329a11c784202b7e50be04f2a6a2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/380 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1336, 24085, 2408, 10, 3482, 298, 201, 223, 223, 223, 34298, 265, 671, 201, 223, 223, 223, 324, 11772, 309, 580, 1244, 385, 1230, 580, 1537, 994, 1332, 19, 63, 351, 994, 431, 18, 63, 201, 223, 223, 223, 337, 290, 292, 1016, 10, 574, 10, 3482, 5488, 19, 14, 460, 14, 458, 19, 298, 201, 223, 223, 223, 223, 223, 223, 223, 337, 274, 292, 1016, 10, 80, 298, 201, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 297, 2811, 61, 75, 63, 577, 2811, 61, 75, 405, 435, 1367, 201, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 34298, 265, 651, 201, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 324, 5543, 2426, 474, 297, 310, 1340, 341, 5387, 1983, 1670, 1340, 292, 310, 1230, 201, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 2811, 61, 75, 547, 2811, 61, 75, 405, 435, 63, 265, 2811, 61, 75, 405, 435, 547, 2811, 61, 75, 63, 223, 223, 223, 223, 223, 223, 223, 201, 223, 223, 223, 223, 223, 223, 223, 297, 372, 34298, 28, 201, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 324, 17051, 310, 944, 297, 713, 10354, 1091, 1551, 294, 2147, 10638, 201, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 324, 10481, 676, 310, 1230, 341, 1912, 2112, 16, 201, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 332], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 3), (3, 10), (10, 14), (14, 15), (15, 23), (23, 25), (25, 26), (26, 27), (27, 28), (28, 29), (29, 37), (37, 39), (39, 45), (45, 46), (46, 47), (47, 48), (48, 49), (49, 51), (51, 56), (56, 59), (59, 64), (64, 69), (69, 72), (72, 78), (78, 83), (83, 88), (88, 94), (94, 96), (96, 97), (97, 98), (98, 101), (101, 107), (107, 109), (109, 110), (110, 111), (111, 112), (112, 113), (113, 114), (114, 115), (115, 119), (119, 121), (121, 124), (124, 130), (130, 131), (131, 134), (134, 135), (135, 143), (143, 145), (145, 146), (146, 147), (147, 149), (149, 150), (150, 152), (152, 153), (153, 155), (155, 156), (156, 157), (157, 158), (158, 159), (159, 160), (160, 161), (161, 162), (162, 163), (163, 167), (167, 169), (169, 172), (172, 178), (178, 179), (179, 180), (180, 182), (182, 183), (183, 184), (184, 185), (185, 186), (186, 187), (187, 188), (188, 189), (189, 190), (190, 191), (191, 192), (192, 193), (193, 194), (194, 197), (197, 206), (206, 207), (207, 208), (208, 209), (209, 211), (211, 220), (220, 221), (221, 222), (222, 224), (224, 226), (226, 228), (228, 229), (229, 230), (230, 231), (231, 232), (232, 233), (233, 234), (234, 235), (235, 236), (236, 237), (237, 238), (238, 239), (239, 240), (240, 241), (241, 242), (242, 243), (243, 244), (244, 252), (252, 254), (254, 259), (259, 260), (260, 261), (261, 262), (262, 263), (263, 264), (264, 265), (265, 266), (266, 267), (267, 268), (268, 269), (269, 270), (270, 271), (271, 272), (272, 273), (273, 274), (274, 275), (275, 277), (277, 280), (280, 286), (286, 291), (291, 294), (294, 298), (298, 306), (306, 309), (309, 314), (314, 319), (319, 324), (324, 332), (332, 335), (335, 339), (339, 345), (345, 346), (346, 347), (347, 348), (348, 349), (349, 350), (350, 351), (351, 352), (352, 353), (353, 354), (354, 355), (355, 356), (356, 357), (357, 358), (358, 359), (359, 360), (360, 361), (361, 370), (370, 371), (371, 372), (372, 374), (374, 383), (383, 384), (384, 385), (385, 387), (387, 389), (389, 390), (390, 392), (392, 401), (401, 402), (402, 403), (403, 405), (405, 407), (407, 409), (409, 418), (418, 419), (419, 420), (420, 421), (421, 422), (422, 423), (423, 424), (424, 425), (425, 426), (426, 427), (427, 428), (428, 429), (429, 430), (430, 431), (431, 432), (432, 433), (433, 434), (434, 435), (435, 436), (436, 439), (439, 443), (443, 451), (451, 452), (452, 453), (453, 454), (454, 455), (455, 456), (456, 457), (457, 458), (458, 459), (459, 460), (460, 461), (461, 462), (462, 463), (463, 464), (464, 466), (466, 474), (474, 478), (478, 487), (487, 490), (490, 493), (493, 498), (498, 500), (500, 505), (505, 507), (507, 514), (514, 519), (519, 520), (520, 521), (521, 522), (522, 523), (523, 524), (524, 525), (525, 526), (526, 527), (527, 528), (528, 529), (529, 530), (530, 531), (531, 533), (533, 541), (541, 546), (546, 550), (550, 556), (556, 559), (559, 567), (567, 574), (574, 575), (575, 576), (576, 577), (577, 578), (578, 579), (579, 580), (580, 581), (581, 582), (582, 583), (583, 584), (584, 585), (585, 586), (586, 587), (587, 594)], 'ast_ids': [74, 112, 112, 1, 112, 15, -1, -1, -1, -1, 112, 139, 2, -1, -1, -1, -1, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, -1, -1, -1, -1, 113, 112, 31, 112, 1, 112, 1, 112, 15, 175, 155, 175, 155, 37, 175, 15, -1, -1, -1, -1, -1, -1, -1, -1, 113, 112, 31, 112, 1, 112, 15, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 66, 112, 5, 112, 102, 87, 112, 5, 112, 189, 175, 102, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 112, 139, 105, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 112, 5, 112, 102, 112, 5, 112, 189, 175, 102, 139, 112, 5, 112, 189, 175, 102, 112, 5, 112, 102, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 66, 43, 112, 85, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 22, 22, 22, 22, 22, 22, 22, 22, 22, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 188], 'parent_ast_ids': [135, 135, 135, 10, 10, 10, -1, -1, -1, -1, 97, 97, 97, -1, -1, -1, -1, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, -1, -1, -1, -1, 82, 82, 82, 99, 27, 99, 27, 27, 27, 69, 27, 27, 27, 194, 194, 27, -1, -1, -1, -1, -1, -1, -1, -1, 82, 82, 82, 99, 27, 27, 27, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 190, 123, 123, 123, 123, 20, 123, 123, 69, 69, 69, 123, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 97, 97, 97, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 123, 123, 123, 123, 123, 123, 69, 69, 69, 123, 97, 123, 123, 69, 69, 69, 123, 123, 123, 123, 123, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 190, 19, 19, 190, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 190, 190, 190, 190, 190, 190, 190, 190, 190, 190, 190, 190, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 190, 190, 190, 190, 190, 190, 190, 190, 190, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 116]}\n",
      "['def', 'Ġbubble', 'sort', '(', 'elements', '):', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġswapped', 'Ġ=', 'ĠFalse', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ#', 'ĠLoop', 'ing', 'Ġfrom', 'Ġsize', 'Ġof', 'Ġarray', 'Ġfrom', 'Ġlast', 'Ġindex', '[-', '1', ']', 'Ġto', 'Ġindex', 'Ġ[', '0', ']', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġfor', 'Ġn', 'Ġin', 'Ġrange', '(', 'len', '(', 'elements', ')-', '1', ',', 'Ġ0', ',', 'Ġ-', '1', '):', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġfor', 'Ġi', 'Ġin', 'Ġrange', '(', 'n', '):', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġif', 'Ġelements', '[', 'i', ']', 'Ġ>', 'Ġelements', '[', 'i', 'Ġ+', 'Ġ1', ']:', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġswapped', 'Ġ=', 'ĠTrue', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ#', 'Ġsw', 'apping', 'Ġdata', 'Ġif', 'Ġthe', 'Ġelement', 'Ġis', 'Ġless', 'Ġthan', 'Ġnext', 'Ġelement', 'Ġin', 'Ġthe', 'Ġarray', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġelements', '[', 'i', '],', 'Ġelements', '[', 'i', 'Ġ+', 'Ġ1', ']', 'Ġ=', 'Ġelements', '[', 'i', 'Ġ+', 'Ġ1', '],', 'Ġelements', '[', 'i', ']', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġif', 'Ġnot', 'Ġswapped', ':', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ#', 'Ġexiting', 'Ġthe', 'Ġfunction', 'Ġif', 'Ġwe', 'Ġdidn', \"'t\", 'Ġmake', 'Ġa', 'Ġsingle', 'Ġswap', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ#', 'Ġmeaning', 'Ġthat', 'Ġthe', 'Ġarray', 'Ġis', 'Ġalready', 'Ġsorted', '.', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġreturn']\n"
     ]
    }
   ],
   "source": [
    "# test the tokenizer\n",
    "py_tokenizer = CodeTokenizer.from_pretrained(\"microsoft/CodeGPT-small-py\", \"python\")\n",
    "#code = \"def foo():\\n    print('hello world')\"\n",
    "#code =  \"def reserve_free_tcp_port(self):\\n        \\\"\\\"\\\"\\\"\\\"\\\"\\n        Reserve free TCP port to be used by Cloud SQL Proxy\\n        \\\"\\\"\\\"\\\"\\\"\\\"\\n        self.reserved_tcp_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\\n        self.reserved_tcp_socket.bind(('127.0.0.1', 0))\\n        self.sql_proxy_tcp_port = self.reserved_tcp_socket.getsockname()[1]\"\n",
    "\n",
    "code = 'def bubblesort(elements):\\n    swapped = False\\n    # Looping from size of array from last index[-1] to index [0]\\n    for n in range(len(elements)-1, 0, -1):\\n        for i in range(n):\\n            if elements[i] > elements[i + 1]:\\n                swapped = True\\n                # swapping data if the element is less than next element in the array\\n                elements[i], elements[i + 1] = elements[i + 1], elements[i]       \\n        if not swapped:\\n            # exiting the function if we didn\\'t make a single swap\\n            # meaning that the array is already sorted.\\n            return'\n",
    "\n",
    "#print(py_tokenizer.node_types)\n",
    "\n",
    "encoding = py_tokenizer(code)\n",
    "\n",
    "assert \"ast_ids\" in encoding\n",
    "assert \"parent_ast_ids\" in encoding\n",
    "assert len(encoding[\"ast_ids\"]) == len(encoding[\"input_ids\"])\n",
    "assert len(encoding[\"parent_ast_ids\"]) == len(encoding[\"input_ids\"])\n",
    "\n",
    "print(encoding)\n",
    "print(py_tokenizer.tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "258"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(py_tokenizer.tokenizer(code)['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['set', '(', 'false', '<>', 'format_specifier', '[', '**=', 'decorator', 'lambda_parameters', 'import_from_statement', 'parameters', 'break', 'aliased_import', '+=', 'default_parameter', ')', '-=', 'pair', 'nonlocal_statement', 'not_operator', 'comparison_operator', 'delete_statement', 'comment', 'global', '==', 'pass_statement', '<<', 'argument_list', 'module', 'relative_import', 'augmented_assignment', 'in', '<=', '<', '.', 'raise', 'pattern', '-', 'exec', 'block', 'lambda', 'set_comprehension', 'parenthesized_list_splat', 'not', '>>=', 'format_expression', 'nonlocal', '}}', '>>', 'case_pattern', 'match', '@', 'elif', 'import_statement', 'await', 'concatenated_string', 'list_splat_pattern', 'as', 'assert', 'ERROR', 'exec_statement', '^', '!=', 'case', 'attribute', ':=', 'if', 'class_definition', '->', 'binary_operator', '_compound_statement', 'expression', 'tuple', '\"', 'def', '>=', 'keyword_argument', 'string', 'expression_list', 'list_comprehension', 'for_in_clause', 'type', 'for_statement', 'pattern_list', 'interpolation', ':', '{', '>', '*', '^=', 'dotted_name', 'generator_expression', 'parameter', 'as_pattern_target', '/=', 'else_clause', 'type_conversion', 'assignment', 'conditional_expression', 'call', '<<=', 'pass', ']', '}', '|', 'true', 'continue_statement', 'float', 'else', 'import_prefix', 'del', 'boolean_operator', 'identifier', 'for', '//=', 'finally_clause', 'return_statement', 'assert_statement', 'list_splat', '&=', 'as_pattern', 'with', '/', 'subscript', 'print_statement', 'global_statement', 'from', 'parenthesized_expression', 'continue', 'break_statement', 'keyword_separator', 'or', 'dictionary_comprehension', 'dictionary_splat_pattern', 'try_statement', 'function_definition', 'expression_statement', 'except', '|=', '=', 'decorated_definition', 'yield', 'async', 'future_import_statement', 'except_clause', 'raise_statement', 'typed_default_parameter', 'while_statement', 'primary_expression', 'dictionary_splat', '*=', 'typed_parameter', 'with_item', '{{', 'finally', ',', 'import', 'with_statement', '@=', 'escape_sequence', '%=', 'try', 'list_pattern', 'elif_clause', 'while', 'with_clause', 'positional_separator', 'dictionary', 'case_clause', 'named_expression', 'chevron', 'wildcard_import', 'tuple_pattern', 'and', 'slice', 'integer', 'if_clause', '__future__', 'ellipsis', 'none', 'print', 'class', '~', 'list', '_simple_statement', 'is', '//', ';', 'return', '+', 'if_statement', 'match_statement', '%', '&', 'unary_operator', '**']\n",
      "196\n"
     ]
    }
   ],
   "source": [
    "print(py_tokenizer.node_types)\n",
    "print(len(py_tokenizer.node_types))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49b68b9c38357b2088122ab1ddc655dffe83bd2309642e44c64d6d94a1d66aec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

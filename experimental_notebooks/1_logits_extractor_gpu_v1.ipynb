{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from statistics import NormalDist\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.1+cu113'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3478, 0.4097, 0.3891],\n",
      "        [0.7176, 0.8822, 0.0272],\n",
      "        [0.8784, 0.1918, 0.7899],\n",
      "        [0.4318, 0.0703, 0.1718],\n",
      "        [0.5152, 0.7167, 0.1274]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "x = torch.rand(5, 3, \n",
    "               device=device#'cpu'\n",
    "               )\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting tensor to free memory\n",
    "del x\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr 20 19:55:39 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-PCI...  Off  | 00000000:61:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    34W / 250W |      7MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logits Extractor\n",
    ">\n",
    "> Extracting Tensor Logits from a given Neural Code Model @danaderp\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_eleuther( returnModel = False, model_type =  'EleutherAI/gpt-neo-125m'):\n",
    "    ''' Eleuther and Salesforce and Parrot uses the same importation'''\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "    logging.info(\"Tokenizer Loaded\")\n",
    "    if returnModel:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_type)\n",
    "        logging.info(\"Model Loaded\")\n",
    "    else:\n",
    "        model = []\n",
    "    \n",
    "    logging.info(model_type)\n",
    "    return tokenizer, model\n",
    "    \n",
    "    \n",
    "\n",
    "def init_model_args( current_case = 'c1', returnModel = False ): \n",
    "    \n",
    "    code_models = {\n",
    "        'c1':('EleutherAI/gpt-neo-125m', ), # Basic (on Pile) GPT-3/J\n",
    "        'c2':('EleutherAI/gpt-neo-1.3B', ),\n",
    "        'c3':('EleutherAI/gpt-neo-2.7B', ),\n",
    "        'c4':('EleutherAI/gpt-j-6b', ),\n",
    "        'c5':('Salesforce/codegen-350M-nl', ), #Basic (on Pile) codegen\n",
    "        'c6':('Salesforce/codegen-2B-nl', ),\n",
    "        'c7':('Salesforce/codegen-6B-nl', ),\n",
    "        'c8':('Salesforce/codegen-16B-nl', ),\n",
    "        'c9':('codeparrot/codeparrot-small-multi', ), #multi-Language\n",
    "        'c10':('Salesforce/codegen-350M-multi', ),\n",
    "        'c11':('Salesforce/codegen-2B-multi', ),\n",
    "        'c12':('Salesforce/codegen-6B-multi', ),\n",
    "        'c13':('Salesforce/codegen-16B-multi', ),\n",
    "        'c14':('codeparrot/codeparrot-small', ), #mono-Language\n",
    "        'c15':('codeparrot/codeparrot', ),\n",
    "        'c16':('Salesforce/codegen-350M-mono', ),\n",
    "        'c17':('Salesforce/codegen-2B-mono', ),\n",
    "        'c18':('Salesforce/codegen-6B-mono', ),\n",
    "        'c19':('Salesforce/codegen-16B-mono', ),\n",
    "    }\n",
    "    \n",
    "    model_type = code_models[current_case][0]\n",
    "    tokenizer, model =  c_eleuther( returnModel = returnModel,  model_type = model_type ) \n",
    "    \n",
    "    \n",
    "    return model_type, tokenizer, model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Parameters\n",
    "> Loading Models and Testbeds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [WARNING] Hyperparameters changes, please tune them up\n",
    "params = {\n",
    "    'codemodel' : 'c2',\n",
    "    'numpy_files_logits_path': '../datax/c2',\n",
    "    'testbeds_path' : '../datax/testbeds/AstEvalVerticalFiltered.json'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "      <th>ids</th>\n",
       "      <th>m_name</th>\n",
       "      <th>code</th>\n",
       "      <th>ast_errors</th>\n",
       "      <th>n_ast_errors</th>\n",
       "      <th>ast_levels</th>\n",
       "      <th>n_whitespaces_</th>\n",
       "      <th>complexity</th>\n",
       "      <th>nloc</th>\n",
       "      <th>token_counts</th>\n",
       "      <th>n_ast_nodes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>280</td>\n",
       "      <td>[4299, 1057, 62, 29412, 62, 41989, 7, 9288, 62...</td>\n",
       "      <td>c1</td>\n",
       "      <td>def run_python_tests(test_modules, parallelism...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>67</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>120</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   size                                                ids m_name   \n",
       "0   280  [4299, 1057, 62, 29412, 62, 41989, 7, 9288, 62...     c1  \\\n",
       "\n",
       "                                                code ast_errors  n_ast_errors   \n",
       "0  def run_python_tests(test_modules, parallelism...         []             0  \\\n",
       "\n",
       "   ast_levels  n_whitespaces_  complexity  nloc  token_counts  n_ast_nodes  \n",
       "0          13              67           7    15           120          154  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing data loads\n",
    "data_pd = pd.read_json( params['testbeds_path'] )\n",
    "data_pd.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-20 20:15:25,198 : INFO : Tokenizer Loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c90a51e272be439e8864235763eec5eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/5.31G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-20 20:18:18,632 : INFO : Model Loaded\n",
      "2023-04-20 20:18:18,634 : INFO : EleutherAI/gpt-neo-1.3B\n"
     ]
    }
   ],
   "source": [
    "#Uploading Model UnderAnalisys\n",
    "name, tokenizer, model = init_model_args(\n",
    "    current_case = params['codemodel'], \n",
    "    returnModel = True #[WARNING!] Check the parameters before calling it. \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Logits From a Given Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating data tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_prompts_ids = data_pd[data_pd['m_name']==params['codemodel']]['ids'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Casting Integers to Tensor Integers. Make sure the tesor is created in a device\n",
    "#We ignored the parameter attention_mask since we are not using masking here [https://huggingface.co/transformers/v4.10.1/glossary.html#attention-mask]\n",
    "\n",
    "tf_input_ids = [torch.tensor(  input_ids, dtype = torch.int, device=device ) for input_ids in filtered_prompts_ids ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Model to Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoForCausalLM(\n",
       "  (transformer): GPTNeoModel(\n",
       "    (wte): Embedding(50257, 2048)\n",
       "    (wpe): Embedding(2048, 2048)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (12): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (13): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (14): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (15): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (16): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (17): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (18): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (19): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (20): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (21): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (22): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (23): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to( device ) #WARNING, Verify the device before assigning to memory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing Logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_extractor(batch, input, from_index=0):\n",
    "    \"\"\"\n",
    "    Output is the class CausalLMOutputWithPast (https://huggingface.co/transformers/v4.10.1/main_classes/output.html?highlight=causallmoutputwithpast)\"\n",
    "    logits (torch.FloatTensor of shape (batch_size, sequence_length, config.vocab_size)) – Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
    "    The expression i.type(torch.LongTensor).to(device) is for casting labels for the loss\n",
    "    \"\"\"\n",
    "    #Output is in CausalLMOutputWithPast\n",
    "    CODEMODEL =  params['codemodel']\n",
    "\n",
    "    for idx, n in enumerate( range( from_index, len(input), batch) ):\n",
    "        output = [ model( \n",
    "            input_ids = i, \n",
    "            labels = i.type(torch.LongTensor).to(device) \n",
    "            ) for i in input[n:n+batch] ] #Labels must be provided to compute loss\n",
    "    \n",
    "        output_logits = [ o.logits.detach().to('cpu').numpy() for o in output ]  #Logits Extraction\n",
    "        output_loss = np.array([ o.loss.detach().to('cpu').numpy() for o in output ])  #Language modeling loss (for next-token prediction).\n",
    "\n",
    "        #Saving Callbacks\n",
    "        current_batch = idx + (from_index//batch)\n",
    "        for jdx, o_logits in enumerate( output_logits ):\n",
    "            np.save( params['numpy_files_logits_path']+ '/'+ f'logits_tensor[{jdx+n}]_batch[{current_batch}]_model[{CODEMODEL}].npy', o_logits) #Saving LOGITS\n",
    "        np.save( params['numpy_files_logits_path']+ '/'+f'loss_batch[{current_batch}]_model[{CODEMODEL}].npy', output_loss) #Saving LOSS\n",
    "        \n",
    "        logging.info(f\"Batch [{current_batch}] Completed\")\n",
    "        \n",
    "        #Memory Released\n",
    "        for out in output:\n",
    "            del out.logits\n",
    "            torch.cuda.empty_cache()\n",
    "            del out.loss\n",
    "            torch.cuda.empty_cache()\n",
    "        for out in output_logits:\n",
    "            del out\n",
    "            torch.cuda.empty_cache()\n",
    "        for out in output_loss:\n",
    "            del out\n",
    "            torch.cuda.empty_cache()\n",
    "        del output\n",
    "        del output_logits\n",
    "        del output_loss\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../dataxc2/logits_tensor[0]_batch[0]_model[c2].npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/CodeSyntaxConcept/experimental_notebooks/1_logits_extractor_gpu_v1.ipynb Cell 26\u001b[0m in \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f736372617463682f64616e61646572702f436f646553796e746178436f6e63657074222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22686f7374223a227373683a2f2f726f63636f2e63732e776d2e656475227d2c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f736372617463682f64616e61646572702f436f646553796e746178436f6e636570742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d/workspaces/CodeSyntaxConcept/experimental_notebooks/1_logits_extractor_gpu_v1.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m## ACTUAL EXPERIMENT\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f736372617463682f64616e61646572702f436f646553796e746178436f6e63657074222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22686f7374223a227373683a2f2f726f63636f2e63732e776d2e656475227d2c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f736372617463682f64616e61646572702f436f646553796e746178436f6e636570742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d/workspaces/CodeSyntaxConcept/experimental_notebooks/1_logits_extractor_gpu_v1.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m## TIME AND MEMORY CONSUMING\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f736372617463682f64616e61646572702f436f646553796e746178436f6e63657074222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22686f7374223a227373683a2f2f726f63636f2e63732e776d2e656475227d2c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f736372617463682f64616e61646572702f436f646553796e746178436f6e636570742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d/workspaces/CodeSyntaxConcept/experimental_notebooks/1_logits_extractor_gpu_v1.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m logit_extractor(\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f736372617463682f64616e61646572702f436f646553796e746178436f6e63657074222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22686f7374223a227373683a2f2f726f63636f2e63732e776d2e656475227d2c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f736372617463682f64616e61646572702f436f646553796e746178436f6e636570742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d/workspaces/CodeSyntaxConcept/experimental_notebooks/1_logits_extractor_gpu_v1.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     batch \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m, \n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f736372617463682f64616e61646572702f436f646553796e746178436f6e63657074222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22686f7374223a227373683a2f2f726f63636f2e63732e776d2e656475227d2c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f736372617463682f64616e61646572702f436f646553796e746178436f6e636570742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d/workspaces/CodeSyntaxConcept/experimental_notebooks/1_logits_extractor_gpu_v1.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39minput\u001b[39;49m \u001b[39m=\u001b[39;49m tf_input_ids, \n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f736372617463682f64616e61646572702f436f646553796e746178436f6e63657074222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22686f7374223a227373683a2f2f726f63636f2e63732e776d2e656475227d2c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f736372617463682f64616e61646572702f436f646553796e746178436f6e636570742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d/workspaces/CodeSyntaxConcept/experimental_notebooks/1_logits_extractor_gpu_v1.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     from_index\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f736372617463682f64616e61646572702f436f646553796e746178436f6e63657074222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22686f7374223a227373683a2f2f726f63636f2e63732e776d2e656475227d2c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f736372617463682f64616e61646572702f436f646553796e746178436f6e636570742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d/workspaces/CodeSyntaxConcept/experimental_notebooks/1_logits_extractor_gpu_v1.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m )\n",
      "\u001b[1;32m/workspaces/CodeSyntaxConcept/experimental_notebooks/1_logits_extractor_gpu_v1.ipynb Cell 26\u001b[0m in \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f736372617463682f64616e61646572702f436f646553796e746178436f6e63657074222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22686f7374223a227373683a2f2f726f63636f2e63732e776d2e656475227d2c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f736372617463682f64616e61646572702f436f646553796e746178436f6e636570742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d/workspaces/CodeSyntaxConcept/experimental_notebooks/1_logits_extractor_gpu_v1.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m current_batch \u001b[39m=\u001b[39m idx \u001b[39m+\u001b[39m (from_index\u001b[39m/\u001b[39m\u001b[39m/\u001b[39mbatch)\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f736372617463682f64616e61646572702f436f646553796e746178436f6e63657074222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22686f7374223a227373683a2f2f726f63636f2e63732e776d2e656475227d2c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f736372617463682f64616e61646572702f436f646553796e746178436f6e636570742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d/workspaces/CodeSyntaxConcept/experimental_notebooks/1_logits_extractor_gpu_v1.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mfor\u001b[39;00m jdx, o_logits \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m( output_logits ):\n\u001b[0;32m---> <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f736372617463682f64616e61646572702f436f646553796e746178436f6e63657074222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22686f7374223a227373683a2f2f726f63636f2e63732e776d2e656475227d2c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f736372617463682f64616e61646572702f436f646553796e746178436f6e636570742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d/workspaces/CodeSyntaxConcept/experimental_notebooks/1_logits_extractor_gpu_v1.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     np\u001b[39m.\u001b[39;49msave( params[\u001b[39m'\u001b[39;49m\u001b[39mnumpy_files_logits_path\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m+\u001b[39;49mCODEMODEL\u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m+\u001b[39;49m \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlogits_tensor[\u001b[39;49m\u001b[39m{\u001b[39;49;00mjdx\u001b[39m+\u001b[39;49mn\u001b[39m}\u001b[39;49;00m\u001b[39m]_batch[\u001b[39;49m\u001b[39m{\u001b[39;49;00mcurrent_batch\u001b[39m}\u001b[39;49;00m\u001b[39m]_model[\u001b[39;49m\u001b[39m{\u001b[39;49;00mCODEMODEL\u001b[39m}\u001b[39;49;00m\u001b[39m].npy\u001b[39;49m\u001b[39m'\u001b[39;49m, o_logits) \u001b[39m#Saving LOGITS\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f736372617463682f64616e61646572702f436f646553796e746178436f6e63657074222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22686f7374223a227373683a2f2f726f63636f2e63732e776d2e656475227d2c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f736372617463682f64616e61646572702f436f646553796e746178436f6e636570742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d/workspaces/CodeSyntaxConcept/experimental_notebooks/1_logits_extractor_gpu_v1.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m np\u001b[39m.\u001b[39msave( params[\u001b[39m'\u001b[39m\u001b[39mnumpy_files_logits_path\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m+\u001b[39mCODEMODEL\u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mloss_batch[\u001b[39m\u001b[39m{\u001b[39;00mcurrent_batch\u001b[39m}\u001b[39;00m\u001b[39m]_model[\u001b[39m\u001b[39m{\u001b[39;00mCODEMODEL\u001b[39m}\u001b[39;00m\u001b[39m].npy\u001b[39m\u001b[39m'\u001b[39m, output_loss) \u001b[39m#Saving LOSS\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f736372617463682f64616e61646572702f436f646553796e746178436f6e63657074222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22686f7374223a227373683a2f2f726f63636f2e63732e776d2e656475227d2c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f736372617463682f64616e61646572702f436f646553796e746178436f6e636570742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d/workspaces/CodeSyntaxConcept/experimental_notebooks/1_logits_extractor_gpu_v1.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBatch [\u001b[39m\u001b[39m{\u001b[39;00mcurrent_batch\u001b[39m}\u001b[39;00m\u001b[39m] Completed\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msave\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/lib/npyio.py:498\u001b[0m, in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    496\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m file\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.npy\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    497\u001b[0m         file \u001b[39m=\u001b[39m file \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.npy\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 498\u001b[0m     file_ctx \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(file, \u001b[39m\"\u001b[39;49m\u001b[39mwb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    500\u001b[0m \u001b[39mwith\u001b[39;00m file_ctx \u001b[39mas\u001b[39;00m fid:\n\u001b[1;32m    501\u001b[0m     arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masanyarray(arr)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../dataxc2/logits_tensor[0]_batch[0]_model[c2].npy'"
     ]
    }
   ],
   "source": [
    "## ACTUAL EXPERIMENT\n",
    "## TIME AND MEMORY CONSUMING\n",
    "logit_extractor(\n",
    "    batch = 1, \n",
    "    input = tf_input_ids, \n",
    "    from_index=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logit_extractor(batch =2, input= input_ids_list[:2]) #<---- [WARNING TIME AND MEMORY CONSUMING]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_logits = np.load('../data/callbacks/logits_tensor[0]_batch[0].npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert output_logits.shape[0] == len(input_ids_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_loss = np.load('../data/callbacks/loss_batch[0].npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

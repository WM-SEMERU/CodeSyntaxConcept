{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from statistics import NormalDist\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.1+cu113'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8523, 0.7189, 0.1888],\n",
      "        [0.4532, 0.9695, 0.1338],\n",
      "        [0.8879, 0.3797, 0.7802],\n",
      "        [0.3420, 0.1448, 0.5591],\n",
      "        [0.5354, 0.4235, 0.6895]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "x = torch.rand(5, 3, \n",
    "               device=device#'cpu'\n",
    "               )\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting tensor to free memory\n",
    "del x\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Feb 20 23:24:52 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-PCI...  Off  | 00000000:61:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    35W / 250W |    994MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logits Extractor\n",
    ">\n",
    "> Extracting Tensor Logits from a given Neural Code Model @danaderp\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Available Datasets\n",
    "# Case1: codesearch_tesbed_EleutherAI-gpt-neo-125M_10000 for the model 'EleutherAI/gpt-neo-125M' \n",
    "# Case2: codesearch_tesbed_EleutherAI-gpt-neo-2.7B_10000 for the model 'EleutherAI/gpt-neo-2.7B' <-- MEMORY CONSTRAINTS\n",
    "\n",
    "def params(): \n",
    "    \n",
    "    code_models = {\n",
    "        'Case3':('EleutherAI/gpt-neo-1.3B','codesearch_tesbed_EleutherAI-gpt-neo-1.3B_10000.csv','EleutherAI-gpt-neo-1.3B_10000_','callbacks-EleutherAI-gpt-neo-1.3B_10000_'),\n",
    "        'Case4':('microsoft/CodeGPT-small-py','codesearch_tesbed_microsoft-CodeGPT-small-py_1024_10000.csv','CodeGPT-small-py_10000_','callbacks-CodeGPT-small-py_10000_'),\n",
    "        'Case5':('microsoft/CodeGPT-small-py-adaptedGPT2','codesearch_tesbed_microsoft-CodeGPT-small-py-adaptedGPT2_1024_10000.csv','CodeGPT-small-py-adaptedGPT2_10000_','callbacks-CodeGPT-small-py-adaptedGPT2_10000_'),\n",
    "        'Case6':('Salesforce/codegen-2B-multi','codesearch_tesbed_Salesforce-codegen-2B-multi_10000.csv','Salesforce-codegen-2B-multi_10000_','callbacks-Salesforce-codegen-2B-multi_10000_')\n",
    "    }\n",
    "    current_case = 'Case5' #<----[Hyper]\n",
    "    \n",
    "    #print(code_models[current_case][1])\n",
    "    \n",
    "    return {\n",
    "            'big_table_path' : '../data/concept_tables/' + code_models[current_case][1],\n",
    "            'hf_model' :  code_models[current_case][0],\n",
    "            'model_name': code_models[current_case][2],\n",
    "            'callbacks' : '../data/' + code_models[current_case][3],\n",
    "            'wpe':1024  #<----[Hyper]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/concept_tables/codesearch_tesbed_microsoft-CodeGPT-small-py-adaptedGPT2_1024_10000.csv'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pwd\n",
    "parameters = params()\n",
    "parameters['big_table_path']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd = pd.read_csv( \n",
    "                      parameters['big_table_path'] , \n",
    "                      index_col=0\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_total_input_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>334.360400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>224.736459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>43.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>161.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>268.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>451.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1023.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model_total_input_ids\n",
       "count           10000.000000\n",
       "mean              334.360400\n",
       "std               224.736459\n",
       "min                43.000000\n",
       "25%               161.000000\n",
       "50%               268.000000\n",
       "75%               451.000000\n",
       "max              1023.000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pd.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>whole_func_string</th>\n",
       "      <th>ast_concepts</th>\n",
       "      <th>model_tokenizer_concepts</th>\n",
       "      <th>model_input_ids</th>\n",
       "      <th>model_total_input_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>def register_context_middleware(self, *middlew...</td>\n",
       "      <td>[('def', 'def', 'function_definition'), ('regi...</td>\n",
       "      <td>[(4299, 'def', 'function_definition'), (7881, ...</td>\n",
       "      <td>[4299, 7881, 62, 22866, 62, 27171, 1574, 7, 94...</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>def parse_iso_8601_time_str(time_str):\\n    \"\"...</td>\n",
       "      <td>[('def', 'def', 'function_definition'), ('pars...</td>\n",
       "      <td>[(4299, 'def', 'function_definition'), (21136,...</td>\n",
       "      <td>[4299, 21136, 62, 26786, 62, 4521, 486, 62, 24...</td>\n",
       "      <td>292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>def load_client_ca(self, cafile):\\n        \"\"\"...</td>\n",
       "      <td>[('def', 'def', 'function_definition'), ('load...</td>\n",
       "      <td>[(4299, 'def', 'function_definition'), (3440, ...</td>\n",
       "      <td>[4299, 3440, 62, 16366, 62, 6888, 7, 944, 11, ...</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>def midi_to_note(midi, octave=True, cents=Fals...</td>\n",
       "      <td>[('def', 'def', 'function_definition'), ('midi...</td>\n",
       "      <td>[(4299, 'def', 'function_definition'), (3095, ...</td>\n",
       "      <td>[4299, 3095, 72, 62, 1462, 62, 11295, 7, 13602...</td>\n",
       "      <td>871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>def validate_response_type(self, client_id, re...</td>\n",
       "      <td>[('def', 'def', 'function_definition'), ('vali...</td>\n",
       "      <td>[(4299, 'def', 'function_definition'), (26571,...</td>\n",
       "      <td>[4299, 26571, 62, 26209, 62, 4906, 7, 944, 11,...</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   whole_func_string  \\\n",
       "0  def register_context_middleware(self, *middlew...   \n",
       "1  def parse_iso_8601_time_str(time_str):\\n    \"\"...   \n",
       "2  def load_client_ca(self, cafile):\\n        \"\"\"...   \n",
       "3  def midi_to_note(midi, octave=True, cents=Fals...   \n",
       "4  def validate_response_type(self, client_id, re...   \n",
       "\n",
       "                                        ast_concepts  \\\n",
       "0  [('def', 'def', 'function_definition'), ('regi...   \n",
       "1  [('def', 'def', 'function_definition'), ('pars...   \n",
       "2  [('def', 'def', 'function_definition'), ('load...   \n",
       "3  [('def', 'def', 'function_definition'), ('midi...   \n",
       "4  [('def', 'def', 'function_definition'), ('vali...   \n",
       "\n",
       "                            model_tokenizer_concepts  \\\n",
       "0  [(4299, 'def', 'function_definition'), (7881, ...   \n",
       "1  [(4299, 'def', 'function_definition'), (21136,...   \n",
       "2  [(4299, 'def', 'function_definition'), (3440, ...   \n",
       "3  [(4299, 'def', 'function_definition'), (3095, ...   \n",
       "4  [(4299, 'def', 'function_definition'), (26571,...   \n",
       "\n",
       "                                     model_input_ids  model_total_input_ids  \n",
       "0  [4299, 7881, 62, 22866, 62, 27171, 1574, 7, 94...                    140  \n",
       "1  [4299, 21136, 62, 26786, 62, 4521, 486, 62, 24...                    292  \n",
       "2  [4299, 3440, 62, 16366, 62, 6888, 7, 944, 11, ...                    241  \n",
       "3  [4299, 3095, 72, 62, 1462, 62, 11295, 7, 13602...                    871  \n",
       "4  [4299, 26571, 62, 26209, 62, 4906, 7, 944, 11,...                    242  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pd.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-2B-mono\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "#generator = pipeline('text-generation', model='EleutherAI/gpt-neo-1.3B')\n",
    "#model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "\n",
    "def testing_1():\n",
    "    #! pip install transformers\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    from transformers import pipeline\n",
    "    # This is for 'EleutherAI/gpt-neo-125M'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(parameters['hf_model'])\n",
    "    generator = pipeline(\n",
    "        'text-generation', \n",
    "        model= parameters['hf_model'] )\n",
    "    \n",
    "    #TEST: Example 1: generation\n",
    "    generator(\n",
    "        \"EleutherAI has\", \n",
    "        do_sample=True, \n",
    "        max_new_tokens=20, \n",
    "        pad_token_id = tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    #TEST: Example 2: generation\n",
    "\n",
    "    generator( \n",
    "        data_pd.whole_func_string.values[0][:100], #<-- Code data\n",
    "        do_sample=True, \n",
    "        max_new_tokens=20,\n",
    "        pad_token_id = tokenizer.eos_token_id \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Logits From a Given Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code works for GPT-Neo\n",
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "model = GPTNeoForCausalLM.from_pretrained( parameters['hf_model'] )\n",
    "tokenizer = GPT2Tokenizer.from_pretrained( parameters['hf_model'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#This code works for CodeGPT and Salesforce\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(parameters['hf_model'])\n",
    "model = AutoModelForCausalLM.from_pretrained(parameters['hf_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = data_pd.whole_func_string.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making space for the tensors\n",
    "input_ids_list = tokenizer.batch_encode_plus( list(prompts) ) #<-- Do not return as a Tensor\n",
    "#input_ids_list = tokenizer.batch_encode_plus( prompts , return_tensors=\"pt\") #<-- \"pt\" returns as a Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Casting Integers to Tensor Integers. Make sure the tesor is created in a device\n",
    "#We ignored the parameter attention_mask since we are not using masking here [https://huggingface.co/transformers/v4.10.1/glossary.html#attention-mask]\n",
    "\n",
    "#input_ids_list = [torch.Tensor( np.array( input_ids ) ) for input_ids in input_ids_list.input_ids if len(input_ids) <= parameters['wpe']]\n",
    "#input_ids_list = [ input_ids for input_ids in input_ids_list.input_ids if len(input_ids) <= parameters['wpe']]\n",
    "input_ids_list = [torch.tensor(  input_ids, dtype = torch.int, device=device ) for input_ids in input_ids_list.input_ids if len(input_ids) <= parameters['wpe']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It should be same size\n",
    "assert len(input_ids_list) == len(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50260, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50260, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.to( 'cpu' ) #WARNING, \n",
    "model.to( device ) #WARNING, Verify the device before assigning to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'big_table_path': '../data/concept_tables/codesearch_tesbed_microsoft-CodeGPT-small-py-adaptedGPT2_1024_10000.csv',\n",
       " 'hf_model': 'microsoft/CodeGPT-small-py-adaptedGPT2',\n",
       " 'model_name': 'CodeGPT-small-py-adaptedGPT2_10000_',\n",
       " 'callbacks': '../data/callbacks-CodeGPT-small-py-adaptedGPT2_10000_',\n",
       " 'wpe': 1024}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters #Verification Point of Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_extractor(batch, input, from_index=0):\n",
    "    \"\"\"\n",
    "    Output is the class CausalLMOutputWithPast (https://huggingface.co/transformers/v4.10.1/main_classes/output.html?highlight=causallmoutputwithpast)\"\n",
    "    logits (torch.FloatTensor of shape (batch_size, sequence_length, config.vocab_size)) – Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
    "    The expression i.type(torch.LongTensor).to(device) is for casting labels for the loss\n",
    "    \"\"\"\n",
    "    #Output is in CausalLMOutputWithPast\n",
    "\n",
    "    for idx, n in enumerate( range( from_index, len(input), batch) ):\n",
    "        output = [ model( \n",
    "            input_ids = i, \n",
    "            labels = i.type(torch.LongTensor).to(device) \n",
    "            ) for i in input[n:n+batch] ] #Labels must be provided to compute loss\n",
    "    \n",
    "        output_logits = [ o.logits.detach().to('cpu').numpy() for o in output ]  #Logits Extraction\n",
    "        output_loss = np.array([ o.loss.detach().to('cpu').numpy() for o in output ])  #Language modeling loss (for next-token prediction).\n",
    "\n",
    "        #Saving Callbacks\n",
    "        current_batch = idx + (from_index//batch)\n",
    "        for jdx, o_logits in enumerate( output_logits ):\n",
    "            np.save( parameters['callbacks']+ '/'+parameters['model_name']  + f'_logits_tensor[{jdx+n}]_batch[{current_batch}].npy', o_logits)\n",
    "        np.save( parameters['callbacks']+ '/'+parameters['model_name']+f'_loss_batch[{current_batch}].npy', output_loss)\n",
    "        \n",
    "        print(f\"Batch [{current_batch}] Completed\")\n",
    "        \n",
    "        #Memory Released\n",
    "        for out in output:\n",
    "            del out.logits\n",
    "            torch.cuda.empty_cache()\n",
    "            del out.loss\n",
    "            torch.cuda.empty_cache()\n",
    "        for out in output_logits:\n",
    "            del out\n",
    "            torch.cuda.empty_cache()\n",
    "        for out in output_loss:\n",
    "            del out\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [0] Completed\n",
      "Batch [1] Completed\n",
      "Batch [2] Completed\n",
      "Batch [3] Completed\n",
      "Batch [4] Completed\n",
      "Batch [5] Completed\n",
      "Batch [6] Completed\n",
      "Batch [7] Completed\n",
      "Batch [8] Completed\n",
      "Batch [9] Completed\n",
      "Batch [10] Completed\n",
      "Batch [11] Completed\n",
      "Batch [12] Completed\n",
      "Batch [13] Completed\n",
      "Batch [14] Completed\n",
      "Batch [15] Completed\n",
      "Batch [16] Completed\n",
      "Batch [17] Completed\n",
      "Batch [18] Completed\n",
      "Batch [19] Completed\n",
      "Batch [20] Completed\n",
      "Batch [21] Completed\n",
      "Batch [22] Completed\n",
      "Batch [23] Completed\n",
      "Batch [24] Completed\n",
      "Batch [25] Completed\n",
      "Batch [26] Completed\n",
      "Batch [27] Completed\n",
      "Batch [28] Completed\n",
      "Batch [29] Completed\n",
      "Batch [30] Completed\n",
      "Batch [31] Completed\n",
      "Batch [32] Completed\n",
      "Batch [33] Completed\n",
      "Batch [34] Completed\n",
      "Batch [35] Completed\n",
      "Batch [36] Completed\n",
      "Batch [37] Completed\n",
      "Batch [38] Completed\n",
      "Batch [39] Completed\n",
      "Batch [40] Completed\n",
      "Batch [41] Completed\n",
      "Batch [42] Completed\n",
      "Batch [43] Completed\n",
      "Batch [44] Completed\n",
      "Batch [45] Completed\n",
      "Batch [46] Completed\n",
      "Batch [47] Completed\n",
      "Batch [48] Completed\n",
      "Batch [49] Completed\n",
      "Batch [50] Completed\n",
      "Batch [51] Completed\n",
      "Batch [52] Completed\n",
      "Batch [53] Completed\n",
      "Batch [54] Completed\n",
      "Batch [55] Completed\n",
      "Batch [56] Completed\n",
      "Batch [57] Completed\n",
      "Batch [58] Completed\n",
      "Batch [59] Completed\n",
      "Batch [60] Completed\n",
      "Batch [61] Completed\n",
      "Batch [62] Completed\n",
      "Batch [63] Completed\n",
      "Batch [64] Completed\n",
      "Batch [65] Completed\n",
      "Batch [66] Completed\n",
      "Batch [67] Completed\n",
      "Batch [68] Completed\n",
      "Batch [69] Completed\n",
      "Batch [70] Completed\n",
      "Batch [71] Completed\n",
      "Batch [72] Completed\n",
      "Batch [73] Completed\n",
      "Batch [74] Completed\n",
      "Batch [75] Completed\n",
      "Batch [76] Completed\n",
      "Batch [77] Completed\n",
      "Batch [78] Completed\n",
      "Batch [79] Completed\n",
      "Batch [80] Completed\n",
      "Batch [81] Completed\n",
      "Batch [82] Completed\n",
      "Batch [83] Completed\n",
      "Batch [84] Completed\n",
      "Batch [85] Completed\n",
      "Batch [86] Completed\n",
      "Batch [87] Completed\n",
      "Batch [88] Completed\n",
      "Batch [89] Completed\n",
      "Batch [90] Completed\n",
      "Batch [91] Completed\n",
      "Batch [92] Completed\n",
      "Batch [93] Completed\n",
      "Batch [94] Completed\n",
      "Batch [95] Completed\n",
      "Batch [96] Completed\n",
      "Batch [97] Completed\n",
      "Batch [98] Completed\n",
      "Batch [99] Completed\n",
      "Batch [100] Completed\n",
      "Batch [101] Completed\n",
      "Batch [102] Completed\n",
      "Batch [103] Completed\n",
      "Batch [104] Completed\n",
      "Batch [105] Completed\n",
      "Batch [106] Completed\n",
      "Batch [107] Completed\n",
      "Batch [108] Completed\n",
      "Batch [109] Completed\n",
      "Batch [110] Completed\n",
      "Batch [111] Completed\n",
      "Batch [112] Completed\n",
      "Batch [113] Completed\n",
      "Batch [114] Completed\n",
      "Batch [115] Completed\n",
      "Batch [116] Completed\n",
      "Batch [117] Completed\n",
      "Batch [118] Completed\n",
      "Batch [119] Completed\n",
      "Batch [120] Completed\n",
      "Batch [121] Completed\n",
      "Batch [122] Completed\n",
      "Batch [123] Completed\n",
      "Batch [124] Completed\n",
      "Batch [125] Completed\n",
      "Batch [126] Completed\n",
      "Batch [127] Completed\n",
      "Batch [128] Completed\n",
      "Batch [129] Completed\n",
      "Batch [130] Completed\n",
      "Batch [131] Completed\n",
      "Batch [132] Completed\n",
      "Batch [133] Completed\n",
      "Batch [134] Completed\n",
      "Batch [135] Completed\n",
      "Batch [136] Completed\n",
      "Batch [137] Completed\n",
      "Batch [138] Completed\n",
      "Batch [139] Completed\n",
      "Batch [140] Completed\n",
      "Batch [141] Completed\n",
      "Batch [142] Completed\n",
      "Batch [143] Completed\n",
      "Batch [144] Completed\n",
      "Batch [145] Completed\n",
      "Batch [146] Completed\n",
      "Batch [147] Completed\n",
      "Batch [148] Completed\n",
      "Batch [149] Completed\n",
      "Batch [150] Completed\n",
      "Batch [151] Completed\n",
      "Batch [152] Completed\n",
      "Batch [153] Completed\n",
      "Batch [154] Completed\n",
      "Batch [155] Completed\n",
      "Batch [156] Completed\n",
      "Batch [157] Completed\n",
      "Batch [158] Completed\n",
      "Batch [159] Completed\n",
      "Batch [160] Completed\n",
      "Batch [161] Completed\n",
      "Batch [162] Completed\n",
      "Batch [163] Completed\n",
      "Batch [164] Completed\n",
      "Batch [165] Completed\n",
      "Batch [166] Completed\n",
      "Batch [167] Completed\n",
      "Batch [168] Completed\n",
      "Batch [169] Completed\n",
      "Batch [170] Completed\n",
      "Batch [171] Completed\n",
      "Batch [172] Completed\n",
      "Batch [173] Completed\n",
      "Batch [174] Completed\n",
      "Batch [175] Completed\n",
      "Batch [176] Completed\n",
      "Batch [177] Completed\n",
      "Batch [178] Completed\n",
      "Batch [179] Completed\n",
      "Batch [180] Completed\n",
      "Batch [181] Completed\n",
      "Batch [182] Completed\n",
      "Batch [183] Completed\n",
      "Batch [184] Completed\n",
      "Batch [185] Completed\n",
      "Batch [186] Completed\n",
      "Batch [187] Completed\n",
      "Batch [188] Completed\n",
      "Batch [189] Completed\n",
      "Batch [190] Completed\n",
      "Batch [191] Completed\n",
      "Batch [192] Completed\n",
      "Batch [193] Completed\n",
      "Batch [194] Completed\n",
      "Batch [195] Completed\n",
      "Batch [196] Completed\n",
      "Batch [197] Completed\n",
      "Batch [198] Completed\n",
      "Batch [199] Completed\n",
      "Batch [200] Completed\n",
      "Batch [201] Completed\n",
      "Batch [202] Completed\n",
      "Batch [203] Completed\n",
      "Batch [204] Completed\n",
      "Batch [205] Completed\n",
      "Batch [206] Completed\n",
      "Batch [207] Completed\n",
      "Batch [208] Completed\n",
      "Batch [209] Completed\n",
      "Batch [210] Completed\n",
      "Batch [211] Completed\n",
      "Batch [212] Completed\n",
      "Batch [213] Completed\n",
      "Batch [214] Completed\n",
      "Batch [215] Completed\n",
      "Batch [216] Completed\n",
      "Batch [217] Completed\n",
      "Batch [218] Completed\n",
      "Batch [219] Completed\n",
      "Batch [220] Completed\n",
      "Batch [221] Completed\n",
      "Batch [222] Completed\n",
      "Batch [223] Completed\n",
      "Batch [224] Completed\n",
      "Batch [225] Completed\n",
      "Batch [226] Completed\n",
      "Batch [227] Completed\n",
      "Batch [228] Completed\n",
      "Batch [229] Completed\n",
      "Batch [230] Completed\n",
      "Batch [231] Completed\n",
      "Batch [232] Completed\n",
      "Batch [233] Completed\n",
      "Batch [234] Completed\n",
      "Batch [235] Completed\n",
      "Batch [236] Completed\n",
      "Batch [237] Completed\n",
      "Batch [238] Completed\n",
      "Batch [239] Completed\n",
      "Batch [240] Completed\n",
      "Batch [241] Completed\n",
      "Batch [242] Completed\n",
      "Batch [243] Completed\n",
      "Batch [244] Completed\n",
      "Batch [245] Completed\n",
      "Batch [246] Completed\n",
      "Batch [247] Completed\n",
      "Batch [248] Completed\n",
      "Batch [249] Completed\n",
      "Batch [250] Completed\n",
      "Batch [251] Completed\n",
      "Batch [252] Completed\n",
      "Batch [253] Completed\n",
      "Batch [254] Completed\n",
      "Batch [255] Completed\n",
      "Batch [256] Completed\n",
      "Batch [257] Completed\n",
      "Batch [258] Completed\n",
      "Batch [259] Completed\n",
      "Batch [260] Completed\n",
      "Batch [261] Completed\n",
      "Batch [262] Completed\n",
      "Batch [263] Completed\n",
      "Batch [264] Completed\n",
      "Batch [265] Completed\n",
      "Batch [266] Completed\n",
      "Batch [267] Completed\n",
      "Batch [268] Completed\n",
      "Batch [269] Completed\n",
      "Batch [270] Completed\n",
      "Batch [271] Completed\n",
      "Batch [272] Completed\n",
      "Batch [273] Completed\n",
      "Batch [274] Completed\n",
      "Batch [275] Completed\n",
      "Batch [276] Completed\n",
      "Batch [277] Completed\n",
      "Batch [278] Completed\n",
      "Batch [279] Completed\n",
      "Batch [280] Completed\n",
      "Batch [281] Completed\n",
      "Batch [282] Completed\n",
      "Batch [283] Completed\n",
      "Batch [284] Completed\n",
      "Batch [285] Completed\n",
      "Batch [286] Completed\n",
      "Batch [287] Completed\n",
      "Batch [288] Completed\n",
      "Batch [289] Completed\n",
      "Batch [290] Completed\n",
      "Batch [291] Completed\n",
      "Batch [292] Completed\n",
      "Batch [293] Completed\n",
      "Batch [294] Completed\n",
      "Batch [295] Completed\n",
      "Batch [296] Completed\n",
      "Batch [297] Completed\n",
      "Batch [298] Completed\n",
      "Batch [299] Completed\n",
      "Batch [300] Completed\n",
      "Batch [301] Completed\n",
      "Batch [302] Completed\n",
      "Batch [303] Completed\n",
      "Batch [304] Completed\n",
      "Batch [305] Completed\n",
      "Batch [306] Completed\n",
      "Batch [307] Completed\n",
      "Batch [308] Completed\n",
      "Batch [309] Completed\n",
      "Batch [310] Completed\n",
      "Batch [311] Completed\n",
      "Batch [312] Completed\n",
      "Batch [313] Completed\n",
      "Batch [314] Completed\n",
      "Batch [315] Completed\n",
      "Batch [316] Completed\n",
      "Batch [317] Completed\n",
      "Batch [318] Completed\n",
      "Batch [319] Completed\n",
      "Batch [320] Completed\n",
      "Batch [321] Completed\n",
      "Batch [322] Completed\n",
      "Batch [323] Completed\n",
      "Batch [324] Completed\n",
      "Batch [325] Completed\n",
      "Batch [326] Completed\n",
      "Batch [327] Completed\n",
      "Batch [328] Completed\n",
      "Batch [329] Completed\n",
      "Batch [330] Completed\n",
      "Batch [331] Completed\n",
      "Batch [332] Completed\n",
      "Batch [333] Completed\n",
      "Batch [334] Completed\n",
      "Batch [335] Completed\n",
      "Batch [336] Completed\n",
      "Batch [337] Completed\n",
      "Batch [338] Completed\n",
      "Batch [339] Completed\n",
      "Batch [340] Completed\n",
      "Batch [341] Completed\n",
      "Batch [342] Completed\n",
      "Batch [343] Completed\n",
      "Batch [344] Completed\n",
      "Batch [345] Completed\n",
      "Batch [346] Completed\n",
      "Batch [347] Completed\n",
      "Batch [348] Completed\n",
      "Batch [349] Completed\n",
      "Batch [350] Completed\n",
      "Batch [351] Completed\n",
      "Batch [352] Completed\n",
      "Batch [353] Completed\n",
      "Batch [354] Completed\n",
      "Batch [355] Completed\n",
      "Batch [356] Completed\n",
      "Batch [357] Completed\n",
      "Batch [358] Completed\n",
      "Batch [359] Completed\n",
      "Batch [360] Completed\n",
      "Batch [361] Completed\n",
      "Batch [362] Completed\n",
      "Batch [363] Completed\n",
      "Batch [364] Completed\n",
      "Batch [365] Completed\n",
      "Batch [366] Completed\n",
      "Batch [367] Completed\n",
      "Batch [368] Completed\n",
      "Batch [369] Completed\n",
      "Batch [370] Completed\n",
      "Batch [371] Completed\n",
      "Batch [372] Completed\n",
      "Batch [373] Completed\n",
      "Batch [374] Completed\n",
      "Batch [375] Completed\n",
      "Batch [376] Completed\n",
      "Batch [377] Completed\n",
      "Batch [378] Completed\n",
      "Batch [379] Completed\n",
      "Batch [380] Completed\n",
      "Batch [381] Completed\n",
      "Batch [382] Completed\n",
      "Batch [383] Completed\n",
      "Batch [384] Completed\n",
      "Batch [385] Completed\n",
      "Batch [386] Completed\n",
      "Batch [387] Completed\n",
      "Batch [388] Completed\n",
      "Batch [389] Completed\n",
      "Batch [390] Completed\n",
      "Batch [391] Completed\n",
      "Batch [392] Completed\n",
      "Batch [393] Completed\n",
      "Batch [394] Completed\n",
      "Batch [395] Completed\n",
      "Batch [396] Completed\n",
      "Batch [397] Completed\n",
      "Batch [398] Completed\n",
      "Batch [399] Completed\n",
      "Batch [400] Completed\n",
      "Batch [401] Completed\n",
      "Batch [402] Completed\n",
      "Batch [403] Completed\n",
      "Batch [404] Completed\n",
      "Batch [405] Completed\n",
      "Batch [406] Completed\n",
      "Batch [407] Completed\n",
      "Batch [408] Completed\n",
      "Batch [409] Completed\n",
      "Batch [410] Completed\n",
      "Batch [411] Completed\n",
      "Batch [412] Completed\n",
      "Batch [413] Completed\n",
      "Batch [414] Completed\n",
      "Batch [415] Completed\n",
      "Batch [416] Completed\n",
      "Batch [417] Completed\n",
      "Batch [418] Completed\n",
      "Batch [419] Completed\n",
      "Batch [420] Completed\n",
      "Batch [421] Completed\n",
      "Batch [422] Completed\n",
      "Batch [423] Completed\n",
      "Batch [424] Completed\n",
      "Batch [425] Completed\n",
      "Batch [426] Completed\n",
      "Batch [427] Completed\n",
      "Batch [428] Completed\n",
      "Batch [429] Completed\n",
      "Batch [430] Completed\n",
      "Batch [431] Completed\n",
      "Batch [432] Completed\n",
      "Batch [433] Completed\n",
      "Batch [434] Completed\n",
      "Batch [435] Completed\n",
      "Batch [436] Completed\n",
      "Batch [437] Completed\n",
      "Batch [438] Completed\n",
      "Batch [439] Completed\n",
      "Batch [440] Completed\n",
      "Batch [441] Completed\n",
      "Batch [442] Completed\n",
      "Batch [443] Completed\n",
      "Batch [444] Completed\n",
      "Batch [445] Completed\n",
      "Batch [446] Completed\n",
      "Batch [447] Completed\n",
      "Batch [448] Completed\n",
      "Batch [449] Completed\n",
      "Batch [450] Completed\n",
      "Batch [451] Completed\n",
      "Batch [452] Completed\n",
      "Batch [453] Completed\n",
      "Batch [454] Completed\n",
      "Batch [455] Completed\n",
      "Batch [456] Completed\n",
      "Batch [457] Completed\n",
      "Batch [458] Completed\n",
      "Batch [459] Completed\n",
      "Batch [460] Completed\n",
      "Batch [461] Completed\n",
      "Batch [462] Completed\n",
      "Batch [463] Completed\n",
      "Batch [464] Completed\n",
      "Batch [465] Completed\n",
      "Batch [466] Completed\n",
      "Batch [467] Completed\n",
      "Batch [468] Completed\n",
      "Batch [469] Completed\n",
      "Batch [470] Completed\n",
      "Batch [471] Completed\n",
      "Batch [472] Completed\n",
      "Batch [473] Completed\n",
      "Batch [474] Completed\n",
      "Batch [475] Completed\n",
      "Batch [476] Completed\n",
      "Batch [477] Completed\n",
      "Batch [478] Completed\n",
      "Batch [479] Completed\n",
      "Batch [480] Completed\n",
      "Batch [481] Completed\n",
      "Batch [482] Completed\n",
      "Batch [483] Completed\n",
      "Batch [484] Completed\n",
      "Batch [485] Completed\n",
      "Batch [486] Completed\n",
      "Batch [487] Completed\n",
      "Batch [488] Completed\n",
      "Batch [489] Completed\n",
      "Batch [490] Completed\n",
      "Batch [491] Completed\n",
      "Batch [492] Completed\n",
      "Batch [493] Completed\n",
      "Batch [494] Completed\n",
      "Batch [495] Completed\n",
      "Batch [496] Completed\n",
      "Batch [497] Completed\n",
      "Batch [498] Completed\n",
      "Batch [499] Completed\n",
      "Batch [500] Completed\n",
      "Batch [501] Completed\n",
      "Batch [502] Completed\n",
      "Batch [503] Completed\n",
      "Batch [504] Completed\n",
      "Batch [505] Completed\n",
      "Batch [506] Completed\n",
      "Batch [507] Completed\n",
      "Batch [508] Completed\n",
      "Batch [509] Completed\n",
      "Batch [510] Completed\n",
      "Batch [511] Completed\n",
      "Batch [512] Completed\n",
      "Batch [513] Completed\n",
      "Batch [514] Completed\n",
      "Batch [515] Completed\n",
      "Batch [516] Completed\n",
      "Batch [517] Completed\n",
      "Batch [518] Completed\n",
      "Batch [519] Completed\n",
      "Batch [520] Completed\n",
      "Batch [521] Completed\n",
      "Batch [522] Completed\n",
      "Batch [523] Completed\n",
      "Batch [524] Completed\n",
      "Batch [525] Completed\n",
      "Batch [526] Completed\n",
      "Batch [527] Completed\n",
      "Batch [528] Completed\n",
      "Batch [529] Completed\n",
      "Batch [530] Completed\n",
      "Batch [531] Completed\n",
      "Batch [532] Completed\n",
      "Batch [533] Completed\n",
      "Batch [534] Completed\n",
      "Batch [535] Completed\n",
      "Batch [536] Completed\n",
      "Batch [537] Completed\n",
      "Batch [538] Completed\n",
      "Batch [539] Completed\n",
      "Batch [540] Completed\n",
      "Batch [541] Completed\n",
      "Batch [542] Completed\n",
      "Batch [543] Completed\n",
      "Batch [544] Completed\n",
      "Batch [545] Completed\n",
      "Batch [546] Completed\n",
      "Batch [547] Completed\n",
      "Batch [548] Completed\n",
      "Batch [549] Completed\n",
      "Batch [550] Completed\n",
      "Batch [551] Completed\n",
      "Batch [552] Completed\n",
      "Batch [553] Completed\n",
      "Batch [554] Completed\n",
      "Batch [555] Completed\n",
      "Batch [556] Completed\n",
      "Batch [557] Completed\n",
      "Batch [558] Completed\n",
      "Batch [559] Completed\n",
      "Batch [560] Completed\n",
      "Batch [561] Completed\n",
      "Batch [562] Completed\n",
      "Batch [563] Completed\n",
      "Batch [564] Completed\n",
      "Batch [565] Completed\n",
      "Batch [566] Completed\n",
      "Batch [567] Completed\n",
      "Batch [568] Completed\n",
      "Batch [569] Completed\n",
      "Batch [570] Completed\n",
      "Batch [571] Completed\n",
      "Batch [572] Completed\n",
      "Batch [573] Completed\n",
      "Batch [574] Completed\n",
      "Batch [575] Completed\n",
      "Batch [576] Completed\n",
      "Batch [577] Completed\n",
      "Batch [578] Completed\n",
      "Batch [579] Completed\n",
      "Batch [580] Completed\n",
      "Batch [581] Completed\n",
      "Batch [582] Completed\n",
      "Batch [583] Completed\n",
      "Batch [584] Completed\n",
      "Batch [585] Completed\n",
      "Batch [586] Completed\n",
      "Batch [587] Completed\n",
      "Batch [588] Completed\n",
      "Batch [589] Completed\n",
      "Batch [590] Completed\n",
      "Batch [591] Completed\n",
      "Batch [592] Completed\n",
      "Batch [593] Completed\n",
      "Batch [594] Completed\n"
     ]
    }
   ],
   "source": [
    "## ACTUAL EXPERIMENT\n",
    "## TIME AND MEMORY CONSUMING\n",
    "logit_extractor(batch = 1, input= input_ids_list, from_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logit_extractor(batch =2, input= input_ids_list[:2]) #<---- [WARNING TIME AND MEMORY CONSUMING]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_logits = np.load('../data/callbacks/logits_tensor[0]_batch[0].npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert output_logits.shape[0] == len(input_ids_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_loss = np.load('../data/callbacks/loss_batch[0].npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename=\"../datax/log.log\", format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from statistics import NormalDist\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "x = torch.rand(5, 3, \n",
    "               device=device#'cpu'\n",
    "               )\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting tensor to free memory\n",
    "del x\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logits Extractor\n",
    ">\n",
    "> Extracting Tensor Logits from a given Neural Code Model @danaderp\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_eleuther( model_type, returnModel = False, cache_path = \"../datax/cache\"):\n",
    "    ''' Eleuther and Salesforce and Parrot uses the same importation'''\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_type, cache_dir = cache_path )\n",
    "    model = []\n",
    "    logging.info(\"Tokenizer Loaded\")\n",
    "    if returnModel:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_type, cache_dir = cache_path)\n",
    "        logging.info(\"Model Loaded\")        \n",
    "    \n",
    "    logging.info(model_type)\n",
    "    return tokenizer, model\n",
    "    \n",
    "    \n",
    "\n",
    "def init_model_args( current_case = 'c1', returnModel = False ): \n",
    "    \n",
    "    code_models = {\n",
    "        'c1':('EleutherAI/gpt-neo-125m', ), # Basic (on Pile) GPT-3/J\n",
    "        'c2':('EleutherAI/gpt-neo-1.3B', ),\n",
    "        'c3':('EleutherAI/gpt-neo-2.7B', ),\n",
    "        'c4':('EleutherAI/gpt-j-6b', ),\n",
    "        'c5':('Salesforce/codegen-350M-nl', ), #Basic (on Pile) codegen\n",
    "        'c6':('Salesforce/codegen-2B-nl', ),\n",
    "        'c7':('Salesforce/codegen-6B-nl', ),\n",
    "        'c8':('Salesforce/codegen-16B-nl', ),\n",
    "        'c9':('codeparrot/codeparrot-small-multi', ), #multi-Language\n",
    "        'c10':('Salesforce/codegen-350M-multi', ),\n",
    "        'c11':('Salesforce/codegen-2B-multi', ),\n",
    "        'c12':('Salesforce/codegen-6B-multi', ),\n",
    "        'c13':('Salesforce/codegen-16B-multi', ),\n",
    "        'c14':('codeparrot/codeparrot-small', ), #mono-Language\n",
    "        'c15':('codeparrot/codeparrot', ),\n",
    "        'c16':('Salesforce/codegen-350M-mono', ),\n",
    "        'c17':('Salesforce/codegen-2B-mono', ),\n",
    "        'c18':('Salesforce/codegen-6B-mono', ),\n",
    "        'c19':('Salesforce/codegen-16B-mono', ),\n",
    "    }\n",
    "    \n",
    "    model_type = code_models[current_case][0]\n",
    "    tokenizer, model =  c_eleuther( returnModel = returnModel,  model_type = model_type ) \n",
    "    \n",
    "    \n",
    "    return model_type, tokenizer, model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Parameters\n",
    "> Loading Models and Testbeds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [WARNING] Hyperparameters changes, please tune them up\n",
    "params = {\n",
    "    'codemodel' : 'c1',\n",
    "    'numpy_files_logits_path': '../datax/1_numpy_files_logits/c1',\n",
    "    'testbeds_path' : '../data/testbeds/AstEvalVerticalFiltered.json',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing data loads\n",
    "data_pd = pd.read_json( params['testbeds_path'] )\n",
    "data_pd.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uploading Model UnderAnalisys\n",
    "name, tokenizer, model = init_model_args(\n",
    "    current_case = params['codemodel'], \n",
    "    returnModel = True #[WARNING!] Check the parameters before calling it. \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Logits From a Given Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating data tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_prompts_ids = data_pd[data_pd['m_name']==params['codemodel']]['ids'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Casting Integers to Tensor Integers. Make sure the tesor is created in a device\n",
    "#We ignored the parameter attention_mask since we are not using masking here [https://huggingface.co/transformers/v4.10.1/glossary.html#attention-mask]\n",
    "\n",
    "tf_input_ids = [torch.tensor(  input_ids, dtype = torch.int, device=device ) for input_ids in filtered_prompts_ids ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Model to Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to( device ) #WARNING, Verify the device before assigning to memory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing Logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_folder(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_extractor(batch, input, from_index=0):\n",
    "    \"\"\"\n",
    "    Output is the class CausalLMOutputWithPast (https://huggingface.co/transformers/v4.10.1/main_classes/output.html?highlight=causallmoutputwithpast)\"\n",
    "    logits (torch.FloatTensor of shape (batch_size, sequence_length, config.vocab_size)) â€“ Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
    "    The expression i.type(torch.LongTensor).to(device) is for casting labels for the loss\n",
    "    \"\"\"\n",
    "    #Output is in CausalLMOutputWithPast\n",
    "    CODEMODEL =  params['codemodel']\n",
    "    create_folder(params['numpy_files_logits_path'])\n",
    "    for idx, n in enumerate( range( from_index, len(input), batch) ):\n",
    "        output = [ model( \n",
    "            input_ids = i, \n",
    "            labels = i.type(torch.LongTensor).to(device) \n",
    "            ) for i in input[n:n+batch] ] #Labels must be provided to compute loss\n",
    "    \n",
    "        output_logits = [ o.logits.detach().to('cpu').numpy() for o in output ]  #Logits Extraction\n",
    "        output_loss = np.array([ o.loss.detach().to('cpu').numpy() for o in output ])  #Language modeling loss (for next-token prediction).\n",
    "\n",
    "        #Saving Callbacks\n",
    "        current_batch = idx + (from_index//batch)\n",
    "        for jdx, o_logits in enumerate( output_logits ):\n",
    "            np.save( params['numpy_files_logits_path']+ '/'+ f'logits_tensor[{jdx+n}]_batch[{current_batch}]_model[{CODEMODEL}].npy', o_logits) #Saving LOGITS\n",
    "        np.save( params['numpy_files_logits_path']+ '/'+f'loss_batch[{current_batch}]_model[{CODEMODEL}].npy', output_loss) #Saving LOSS\n",
    "        \n",
    "        logging.info(f\"Batch [{current_batch}] Completed\")\n",
    "        \n",
    "        #Memory Released\n",
    "        for out in output:\n",
    "            del out.logits\n",
    "            torch.cuda.empty_cache()\n",
    "            del out.loss\n",
    "            torch.cuda.empty_cache()\n",
    "        for out in output_logits:\n",
    "            del out\n",
    "            torch.cuda.empty_cache()\n",
    "        for out in output_loss:\n",
    "            del out\n",
    "            torch.cuda.empty_cache()\n",
    "        del output\n",
    "        del output_logits\n",
    "        del output_loss\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ACTUAL EXPERIMENT\n",
    "## TIME AND MEMORY CONSUMING\n",
    "logit_extractor(\n",
    "    batch = 1, \n",
    "    input = tf_input_ids, \n",
    "    from_index=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logit_extractor(batch =2, input= input_ids_list[:2]) #<---- [WARNING TIME AND MEMORY CONSUMING]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_logits = np.load('../data/callbacks/logits_tensor[0]_batch[0].npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert output_logits.shape[0] == len(input_ids_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_loss = np.load('../data/callbacks/loss_batch[0].npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
